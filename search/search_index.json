{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Genomics Extension for SQLite (\"GenomicSQLite\") https://github.com/mlin/GenomicSQLite This SQLite3 loadable extension supports applications in genome bioinformatics by adding: genomic range indexing for overlap queries & joins streaming storage compression using multithreaded Zstandard pre-tuned settings for \"big data\" Our Colab notebook demonstrates key features with Python, one of several language bindings. USE AT YOUR OWN RISK: The extension makes fundamental changes to the database storage layer. While designed to preserve ACID transaction safety, it's young and unlikely to have zero bugs. This project is not associated with the SQLite developers. SQLite \u2265 3.31.0 required To use the Genomics Extension you might first need to upgrade SQLite itself. The host program must link SQLite version 3.31.0 (2020-01-22) or newer. In your shell, sqlite3 --version displays the version installed with your OS, which is probably what your programs use; if in doubt, cause a program to report the result of SELECT sqlite_version() . If this is too old, then upgrade the system/environment SQLite3 library if possible & applicable. Otherwise, modify your program's linking step or runtime environment to cause it to use an up-to-date version, for example by setting rpath or LD_LIBRARY_PATH/DYLD_LIBRARY_PATH to the location of an up-to-date shared library file. Resources: How To Compile SQLite DreamHost Knowledge Base - Installing a custom version of SQLite3 Ubuntu PPA with sqlite3 Rpmfind: libsqlite3 Sqlite :: Anaconda Cloud Homebrew formula/sqlite , formula-linux/sqlite You can always SELECT sqlite_version() to verify the upgrade in your program. Installation It's usually easiest to obtain the extension as a pre-compiled shared library (Linux .so or macOS .dylib), either installed with a package manager, or downloaded from GitHub Releases . Python pip3 install [ --user | --system ] genomicsqlite # -or- conda install -c mlin genomicsqlite The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. JVM Add entries like the following to your Maven pom.xml : <repositories> <repository> <id> genomicsqlite-jdbc </id> <url> https://raw.githubusercontent.com/wiki/mlin/GenomicSQLite/mvn-repo/ </url> </repository> </repositories> <dependencies> <dependency> <groupId> net.mlin </groupId> <artifactId> genomicsqlite-jdbc </artifactId> <version> vX.Y.Z </version> </dependency> </dependencies> Or, download the JAR from GitHub Releases and place it in your classpath, along with sqlite-jdbc 's JAR which is also required. The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. C/C++ Download zip of shared library and genomicsqlite.h from GitHub Releases . Build your program with them, and also ensure the dynamic linker will find the shared library at runtime, by either: (1) installing it in a system or user lib directory & refreshing cache, (2) setting LD_LIBRARY_PATH environment variable, (3) building with -rpath . Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. See our GitHub README for the source build procedure, if needed. We welcome community contributions to the available language bindings; see the Language Bindings Guide if interested. Smoke test We recommend trying our \"smoke test\" script to quickly verify your local system's compatibility. It requires the Python package installed per the instructions above. wget -nv -O - https://raw.githubusercontent.com/mlin/GenomicSQLite/release/test/genomicsqlite_smoke_test.py \\ | python3 - Even if you're not planning to use Python, this test's detailed logs may be useful to diagnose general problems loading the extension. Use cases The extension makes SQLite an efficient foundation for: Integrative genomics data warehouse BED, GFF/GTF, FASTA, FASTQ, SAM, VCF, ... One file, zero administration, portable between platforms and languages Slicing & basic analysis with indexed SQL queries, joins, & aggregations Transactional storage engine for API services, incremental reanalysis, real-time basecalling & metagenomics, ... Experimental new data models, before dedicated storage format & tooling are warranted, if ever. Contraindications Huge numerical arrays: see HDF5 , Zarr , Parquet , Arrow , TileDB . SQLite's BLOB I/O leaves the door open for mash-ups! Parallel SQL analytics / OLAP: see Spark , DuckDB , many commercial products. (Some bases can be covered with a sharding harness for a pool of threads with their own SQLite connections...) Streaming: SQLite storage relies on randomly reading & writing throughout the database file. Proceed to the Programming Guide section to get started building applications.","title":"Intro & Install"},{"location":"#genomics-extension-for-sqlite","text":"","title":"Genomics Extension for SQLite"},{"location":"#genomicsqlite","text":"https://github.com/mlin/GenomicSQLite This SQLite3 loadable extension supports applications in genome bioinformatics by adding: genomic range indexing for overlap queries & joins streaming storage compression using multithreaded Zstandard pre-tuned settings for \"big data\" Our Colab notebook demonstrates key features with Python, one of several language bindings. USE AT YOUR OWN RISK: The extension makes fundamental changes to the database storage layer. While designed to preserve ACID transaction safety, it's young and unlikely to have zero bugs. This project is not associated with the SQLite developers.","title":"(\"GenomicSQLite\")"},{"location":"#sqlite-3310-required","text":"To use the Genomics Extension you might first need to upgrade SQLite itself. The host program must link SQLite version 3.31.0 (2020-01-22) or newer. In your shell, sqlite3 --version displays the version installed with your OS, which is probably what your programs use; if in doubt, cause a program to report the result of SELECT sqlite_version() . If this is too old, then upgrade the system/environment SQLite3 library if possible & applicable. Otherwise, modify your program's linking step or runtime environment to cause it to use an up-to-date version, for example by setting rpath or LD_LIBRARY_PATH/DYLD_LIBRARY_PATH to the location of an up-to-date shared library file. Resources: How To Compile SQLite DreamHost Knowledge Base - Installing a custom version of SQLite3 Ubuntu PPA with sqlite3 Rpmfind: libsqlite3 Sqlite :: Anaconda Cloud Homebrew formula/sqlite , formula-linux/sqlite You can always SELECT sqlite_version() to verify the upgrade in your program.","title":"SQLite &ge; 3.31.0 required"},{"location":"#installation","text":"It's usually easiest to obtain the extension as a pre-compiled shared library (Linux .so or macOS .dylib), either installed with a package manager, or downloaded from GitHub Releases . Python pip3 install [ --user | --system ] genomicsqlite # -or- conda install -c mlin genomicsqlite The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. JVM Add entries like the following to your Maven pom.xml : <repositories> <repository> <id> genomicsqlite-jdbc </id> <url> https://raw.githubusercontent.com/wiki/mlin/GenomicSQLite/mvn-repo/ </url> </repository> </repositories> <dependencies> <dependency> <groupId> net.mlin </groupId> <artifactId> genomicsqlite-jdbc </artifactId> <version> vX.Y.Z </version> </dependency> </dependencies> Or, download the JAR from GitHub Releases and place it in your classpath, along with sqlite-jdbc 's JAR which is also required. The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. C/C++ Download zip of shared library and genomicsqlite.h from GitHub Releases . Build your program with them, and also ensure the dynamic linker will find the shared library at runtime, by either: (1) installing it in a system or user lib directory & refreshing cache, (2) setting LD_LIBRARY_PATH environment variable, (3) building with -rpath . Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. See our GitHub README for the source build procedure, if needed. We welcome community contributions to the available language bindings; see the Language Bindings Guide if interested.","title":"Installation"},{"location":"#smoke-test","text":"We recommend trying our \"smoke test\" script to quickly verify your local system's compatibility. It requires the Python package installed per the instructions above. wget -nv -O - https://raw.githubusercontent.com/mlin/GenomicSQLite/release/test/genomicsqlite_smoke_test.py \\ | python3 - Even if you're not planning to use Python, this test's detailed logs may be useful to diagnose general problems loading the extension.","title":"Smoke test"},{"location":"#use-cases","text":"The extension makes SQLite an efficient foundation for: Integrative genomics data warehouse BED, GFF/GTF, FASTA, FASTQ, SAM, VCF, ... One file, zero administration, portable between platforms and languages Slicing & basic analysis with indexed SQL queries, joins, & aggregations Transactional storage engine for API services, incremental reanalysis, real-time basecalling & metagenomics, ... Experimental new data models, before dedicated storage format & tooling are warranted, if ever.","title":"Use cases"},{"location":"#contraindications","text":"Huge numerical arrays: see HDF5 , Zarr , Parquet , Arrow , TileDB . SQLite's BLOB I/O leaves the door open for mash-ups! Parallel SQL analytics / OLAP: see Spark , DuckDB , many commercial products. (Some bases can be covered with a sharding harness for a pool of threads with their own SQLite connections...) Streaming: SQLite storage relies on randomly reading & writing throughout the database file. Proceed to the Programming Guide section to get started building applications.","title":"Contraindications"},{"location":"bindings/","text":"Language Bindings Guide Thank you for considering a contribution to the language bindings available for the Genomics Extension! Overview The extension is a C++11 library, and C/C++ programs compile against it in the usual way to invoke the routines outlined in the Programming Guide. But for other programming languages, another option should make it unnecessary to use a C/C++ foreign-function interface . The key routines are also exposed as custom SQL functions , which can be invoked by SELECT statements on any SQLite connection, once the extension has been loaded. New language bindings consist largely of ~one-liner functions that pass arguments through to SELECT routine(arg1,arg2,...) and return the result, usually a TEXT value. None are performance-sensitive, as long as developers use prepared, parameterized SQLite3 statements for GRI queries in loops. Bindings should endeavor to integrate \"naturally\" with the host language and its existing SQLite3 bindings. For example, the object returned by the Open procedure should be an idiomatic SQLite3 connection object. Also, APIs should follow the host language's conventions for naming style and optional/keyword arguments. Our Python module can be followed as an illustrative example. Locating & loading the extension library The module should first locate the extension shared-library file (e.g. libgenomicsqlite.so on Linux), but it doesn't actually load it; instead, it tells SQLite to load it . This can occur either during global module initialization or on the first connection attempt. Select the shared-library file in the following order of preference. For development, start with prebuilt binaries from GitHub Releases . Value of LIBGENOMICSQLITE environment variable, if nonempty Platform-appropriate file shipped with the bindings, if you choose to do so (see Packaging, below) Fall back to \"libgenomicsqlite\" to let SQLite use dlopen() to look for it in default locations Use the language SQLite3 bindings to open a connection to a :memory: database , which will just serve these initialization operations. On the connection, enable extension loading if needed and perform sqlite3_load_extension() , or equivalent, on the library filename. The extension needs to be loaded only once per process: upon first loading, it registers itself to activate automatically on each new connection opened. GenomicSQLite Open The Open method should \"look like\" the language's existing wrapper around sqlite3_open() , taking similar arguments and returning the same type of connection object. It uses two routines from the GenomicSQLite library, exposed as SQL functions, which help with activating the compression layer and tuning various settings. Given a database filename, the method follows these steps: 1. Generate connection string with SELECT genomicsqlite_uri(dbfilename, config_json) This helper generates a text URI based on the given filename, which tells SQLite how to open it with the compression layer activated. Call it on your :memory: connection. config_json is the text of a JSON object containing keys and values for the several tuning options shown in the Programming Guide. Any supplied settings are merged into a hard-coded default JSON, so it's only necessary to specify values that need to be overridden, and fine to pass '{}' if there are none. The Open method should allow the caller to supply these optional arguments in some linguistically natural way, then take care of formulating the JSON text. You can access the hard-coded defaults with SELECT genomicsqlite_default_config_json() , which may be useful to determine the available keys. (For example, the Python bindings use this to distinguish the GenomicSQLite options from other optional arguments meant to be passed through to SQLite.) 2. Call sqlite3_open() using the connection string Give the URI connection string to the normal SQLite open method. You must set the SQLITE_OPEN_URI flag or equivalent for this to work. The Open method should pass other flags based on optional arguments in the same way as the existing SQLite3 wrapper. If the caller requested a read-only connection, Open can either set SQLITE_OPEN_READONLY or append &mode=ro to the connection string. 3. Generate tuning script with SELECT genomicsqlite_tuning_sql(config_json) Through the new connection, pass the same config_json described above to this helper. The helper doesn't actually do anything to the database; it merely generates text of an SQL script (semicolon-separated statements) for you to execute. 4. Execute tuning script on the new connection The existing SQLite3 bindings probably expose some method to imperatively execute the semicolon-separated SQL script in one shot. 5. Return the connection object At this point the connection is ready to go, and it should not be necessary to wrap it. Other routines The other routines are much simpler. The binding for each just takes its required and optional arguments, passes them through to a SELECT routine(...) statement on the caller-supplied connection object, and returns the single text answer. SELECT genomicsqlite_version() SELECT genomicsqlite_attach_sql(dbfilename, schema_name, config_json) takes the same config_json as would Open SELECT genomicsqlite_vacuum_into_sql(destfilename, config_json) SELECT create_genomic_range_index_sql(tableName, chromosome, beginPosition, endPosition[, floor]) : floor is an integer, others text. SELECT put_reference_assembly_sql(assembly) SELECT put_reference_sequence_sql(name, length[, assembly, refget_id, meta_json, rid]) length and rid are integers. Optional text arguments can default to NULL, and optional integers can default to -1. The bindings for Get Reference Sequences by Rid just read the _gri_refseq table like, SELECT _gri_rid, gri_refseq_name, gri_refseq_length, gri_assembly, gri_refget_id, gri_refseq_meta_json FROM _gri_refseq and loads the results into some linguistically-natural data structure that'll provide quick lookup of those attributes by rid. Then, Get Reference Sequences by Name can simply call that and \"invert\" the results. Packaging Our GitHub Releases supply prebuilt extension binaries intended to be compatible with most modern hosts. There are at least three packaging options: Bundle nothing: require the end user to download the right library file (or build it themselves) and place it where the bindings and SQLite3 will be able to find them, as described above. Bundle our binaries: ship our binaries inside your package and choose the right one to load at runtime. Remember, the only actual ABI linking occurs between the extension library and SQLite3 itself, so there's nothing to worry about compatibility between the library and the host language runtime. Source build: You can specify the CMake-based source build procedure & its dependencies described in the GitHub README however the host language's packaging system does it. Documentation The MkDocs source Markdown for this documentation site is in the GitHub repository . Fork it & send us a pull request, adding appropriate examples to all the code tabs in the Installation & Programming Guide.","title":"Language Bindings Guide"},{"location":"bindings/#language-bindings-guide","text":"Thank you for considering a contribution to the language bindings available for the Genomics Extension!","title":"Language Bindings Guide"},{"location":"bindings/#overview","text":"The extension is a C++11 library, and C/C++ programs compile against it in the usual way to invoke the routines outlined in the Programming Guide. But for other programming languages, another option should make it unnecessary to use a C/C++ foreign-function interface . The key routines are also exposed as custom SQL functions , which can be invoked by SELECT statements on any SQLite connection, once the extension has been loaded. New language bindings consist largely of ~one-liner functions that pass arguments through to SELECT routine(arg1,arg2,...) and return the result, usually a TEXT value. None are performance-sensitive, as long as developers use prepared, parameterized SQLite3 statements for GRI queries in loops. Bindings should endeavor to integrate \"naturally\" with the host language and its existing SQLite3 bindings. For example, the object returned by the Open procedure should be an idiomatic SQLite3 connection object. Also, APIs should follow the host language's conventions for naming style and optional/keyword arguments. Our Python module can be followed as an illustrative example.","title":"Overview"},{"location":"bindings/#locating-loading-the-extension-library","text":"The module should first locate the extension shared-library file (e.g. libgenomicsqlite.so on Linux), but it doesn't actually load it; instead, it tells SQLite to load it . This can occur either during global module initialization or on the first connection attempt. Select the shared-library file in the following order of preference. For development, start with prebuilt binaries from GitHub Releases . Value of LIBGENOMICSQLITE environment variable, if nonempty Platform-appropriate file shipped with the bindings, if you choose to do so (see Packaging, below) Fall back to \"libgenomicsqlite\" to let SQLite use dlopen() to look for it in default locations Use the language SQLite3 bindings to open a connection to a :memory: database , which will just serve these initialization operations. On the connection, enable extension loading if needed and perform sqlite3_load_extension() , or equivalent, on the library filename. The extension needs to be loaded only once per process: upon first loading, it registers itself to activate automatically on each new connection opened.","title":"Locating &amp; loading the extension library"},{"location":"bindings/#genomicsqlite-open","text":"The Open method should \"look like\" the language's existing wrapper around sqlite3_open() , taking similar arguments and returning the same type of connection object. It uses two routines from the GenomicSQLite library, exposed as SQL functions, which help with activating the compression layer and tuning various settings. Given a database filename, the method follows these steps:","title":"GenomicSQLite Open"},{"location":"bindings/#1-generate-connection-string-with-select-genomicsqlite_uridbfilename-config_json","text":"This helper generates a text URI based on the given filename, which tells SQLite how to open it with the compression layer activated. Call it on your :memory: connection. config_json is the text of a JSON object containing keys and values for the several tuning options shown in the Programming Guide. Any supplied settings are merged into a hard-coded default JSON, so it's only necessary to specify values that need to be overridden, and fine to pass '{}' if there are none. The Open method should allow the caller to supply these optional arguments in some linguistically natural way, then take care of formulating the JSON text. You can access the hard-coded defaults with SELECT genomicsqlite_default_config_json() , which may be useful to determine the available keys. (For example, the Python bindings use this to distinguish the GenomicSQLite options from other optional arguments meant to be passed through to SQLite.)","title":"1. Generate connection string with SELECT genomicsqlite_uri(dbfilename, config_json)"},{"location":"bindings/#2-call-sqlite3_open-using-the-connection-string","text":"Give the URI connection string to the normal SQLite open method. You must set the SQLITE_OPEN_URI flag or equivalent for this to work. The Open method should pass other flags based on optional arguments in the same way as the existing SQLite3 wrapper. If the caller requested a read-only connection, Open can either set SQLITE_OPEN_READONLY or append &mode=ro to the connection string.","title":"2. Call sqlite3_open() using the connection string"},{"location":"bindings/#3-generate-tuning-script-with-select-genomicsqlite_tuning_sqlconfig_json","text":"Through the new connection, pass the same config_json described above to this helper. The helper doesn't actually do anything to the database; it merely generates text of an SQL script (semicolon-separated statements) for you to execute.","title":"3. Generate tuning script with SELECT genomicsqlite_tuning_sql(config_json)"},{"location":"bindings/#4-execute-tuning-script-on-the-new-connection","text":"The existing SQLite3 bindings probably expose some method to imperatively execute the semicolon-separated SQL script in one shot.","title":"4. Execute tuning script on the new connection"},{"location":"bindings/#5-return-the-connection-object","text":"At this point the connection is ready to go, and it should not be necessary to wrap it.","title":"5. Return the connection object"},{"location":"bindings/#other-routines","text":"The other routines are much simpler. The binding for each just takes its required and optional arguments, passes them through to a SELECT routine(...) statement on the caller-supplied connection object, and returns the single text answer. SELECT genomicsqlite_version() SELECT genomicsqlite_attach_sql(dbfilename, schema_name, config_json) takes the same config_json as would Open SELECT genomicsqlite_vacuum_into_sql(destfilename, config_json) SELECT create_genomic_range_index_sql(tableName, chromosome, beginPosition, endPosition[, floor]) : floor is an integer, others text. SELECT put_reference_assembly_sql(assembly) SELECT put_reference_sequence_sql(name, length[, assembly, refget_id, meta_json, rid]) length and rid are integers. Optional text arguments can default to NULL, and optional integers can default to -1. The bindings for Get Reference Sequences by Rid just read the _gri_refseq table like, SELECT _gri_rid, gri_refseq_name, gri_refseq_length, gri_assembly, gri_refget_id, gri_refseq_meta_json FROM _gri_refseq and loads the results into some linguistically-natural data structure that'll provide quick lookup of those attributes by rid. Then, Get Reference Sequences by Name can simply call that and \"invert\" the results.","title":"Other routines"},{"location":"bindings/#packaging","text":"Our GitHub Releases supply prebuilt extension binaries intended to be compatible with most modern hosts. There are at least three packaging options: Bundle nothing: require the end user to download the right library file (or build it themselves) and place it where the bindings and SQLite3 will be able to find them, as described above. Bundle our binaries: ship our binaries inside your package and choose the right one to load at runtime. Remember, the only actual ABI linking occurs between the extension library and SQLite3 itself, so there's nothing to worry about compatibility between the library and the host language runtime. Source build: You can specify the CMake-based source build procedure & its dependencies described in the GitHub README however the host language's packaging system does it.","title":"Packaging"},{"location":"bindings/#documentation","text":"The MkDocs source Markdown for this documentation site is in the GitHub repository . Fork it & send us a pull request, adding appropriate examples to all the code tabs in the Installation & Programming Guide.","title":"Documentation"},{"location":"guide/","text":"Programming Guide The Genomics Extension integrates with your programming language's existing SQLite3 bindings to provide a familiar experience wherever possible. Python: sqlite3 Java/JVM: sqlite-jdbc C++: SQLiteCpp (optional, recommended) or directly using... C: SQLite C/C++ API Loading the extension Python import sqlite3 import genomicsqlite Java import java.sql.* ; import net.mlin.genomicsqlite.GenomicSQLite ; C++ // link program to sqlite3 and genomicsqlite libraries; optionally, SQLiteCpp: // https://github.com/SRombauts/SQLiteCpp #include <sqlite3.h> #include \"SQLiteCpp/SQLiteCpp.h\" // optional #include \"genomicsqlite.h\" // General note: most GenomicSQLite C++ routines are liable to throw. C /* link program to sqlite3 and genomicsqlite libraries */ #include <sqlite3.h> #include \"genomicsqlite.h\" /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */ Opening a compressed database \u21aa GenomicSQLite Open: create or open a compressed database, returning a connection object with various settings pre-tuned for large datasets. Python dbconn = genomicsqlite . connect ( db_filename , read_only = False , ** kwargs # genomicsqlite + sqlite3.connect() arguments ) assert isinstance ( dbconn , sqlite3 . Connection ) Java java . util . Properties config = new java . util . Properties (); config . setProperty ( \"genomicsqlite.config_json\" , \"{}\" ); // Properties may originate from org.sqlite.SQLiteConfig.toProperties() // with genomicsqlite.config_json added in. Connection dbconn = DriverManager . getConnection ( \"jdbc:genomicsqlite:\" + dbfileName , config ); SQLiteCpp std :: unique_ptr < SQLite :: Database > GenomicSQLiteOpen ( const std :: string & db_filename , int flags = 0 , const std :: string & config_json = \"{}\" ); C++ int GenomicSQLiteOpen ( const std :: string & db_filename , sqlite3 ** ppDb , std :: string & errmsg_out , int flags = 0 , // as sqlite3_open_v2 () e . g . SQLITE_OPEN_READONLY const std :: string & config_json = \" {} \" ) noexcept ; // returns sqlite3_open_v2() code C int genomicsqlite_open ( const char * db_filename , sqlite3 ** ppDb , char ** pzErrMsg , /* if nonnull and an error occurs, set to error message * which caller should sqlite3_free() */ int flags , /* as sqlite3_open_v2() e.g. SQLITE_OPEN_READONLY */ const char * config_json /* JSON text (may be null) */ ); /* returns sqlite3_open_v2() code */ Afterwards, all the usual SQLite3 API operations are available through the returned connection object, which should finally be closed in the usual way. The storage compression layer operates transparently underneath. \u2757 GenomicSQLite databases should only be opened using this routine. If a program opens an existing GenomicSQLite database using a generic SQLite3 API, it will find a valid database whose schema is that of the compression layer instead of the intended application's. Writing into that schema might effectively corrupt the database! Tuning options The aforementioned tuned settings can be further adjusted. Some bindings (e.g. C/C++) receive these options as the text of a JSON object with keys and values, while others admit individual arguments to the Open routine. threads = -1 : thread budget for compression, sort, and prefetching/decompression operations; -1 to match up to 8 host processors. Set 1 to disable all background processing. inner_page_KiB = 16 : SQLite page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. Larger pages are more compressible, but increase random I/O cost. outer_page_KiB = 32 : compression layer page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. The default configuration (inner_page_KiB, outer_page_KiB) = (16,32) balances random access speed and compression. Try setting them to (8,16) to prioritize random access, or (64,2) to prioritize compression (if compressed database will be <4TB) . zstd_level = 6 : Zstandard compression level for newly written data (-7 to 22) unsafe_load = false : set true to disable write transaction safety (see advice on bulk-loading below). \u2757 A database written to unsafely is liable to be corrupted if the application crashes, or if there's a concurrent attempt to modify it. page_cache_MiB = 1024 : database cache size. Use a large cache to avoid repeated decompression in successive and complex queries. immutable = false : set true to slightly reduce overhead reading from a database file that won't be modified by this or any concurrent program, guaranteed. force_prefetch = false : set true to enable background prefetching/decompression even if inner_page_KiB < 16 (enabled by default only \u2265 that, as it can be counterproductive below; YMMV) The connection's potential memory usage can usually be budgeted as roughly the page cache size, plus the size of any uncommitted write transaction (unless unsafe_load), plus some safety factor. \u2757However, this can multiply by (threads+1) during queries whose results are at least that large and must be re-sorted. That includes index creation, when the indexed columns total such size. Advice for big data Tips for writing large databases quickly: sqlite3_config(SQLITE_CONFIG_MEMSTATUS, 0) if available, to reduce overhead in SQLite3's allocation routines. Open database with unsafe_load = true to reduce transaction processing overhead (at aforementioned risk) for the connection's lifetime. Also open with the flag SQLITE_OPEN_NOMUTEX , if your application naturally serializes operations on the connection. Perform all of the following steps within one big SQLite transaction, committed at the end. Insert data rows reusing prepared, parameterized SQL statements. For tables with explicit primary keys: insert their rows in primary key order, if feasible. Consider preparing data in producer thread(s), with a consumer thread executing insertion statements in a tight loop. Bind text/blob parameters using SQLITE_STATIC if suitable. Create secondary indices, including genomic range indices, only after loading all row data. Use partial indices when they suffice. Compression guidelines The Zstandard -based compression layer is effective at capturing the typically high compressibility of bioinformatics data. But, one should expect a general-purpose database to need some extra space to keep everything organized, compared to a file format dedicated to one predetermined, read-only schema. To set a rough expectation, the maintainers feel fairly satisfied if the database file size isn't more than double that of a bespoke compression format \u2014 especially if it includes useful indices (which if well-designed, should be relatively incompressible). With SQLite's row-major table storage format , the first read of a lone cell usually entails decompressing at least its whole row, and there aren't any special column encodings for deltas, run lengths, etc. The \"last mile\" of optimization may therefore involve certain schema compromises, such as storing infrequently-accessed columns in a separate table to join when needed, or using application-layer encodings with BLOB I/O . The aforementioned zstd_level, threads, and page_size options all affect the compression time-space tradeoff, while enlarging the page cache can reduce decompression overhead (workload-dependent). Genomic range indexing Overview GenomicSQLite enables creation of a Genomic Range Index (GRI) for any database table in which each row represents a genomic feature with (chromosome, beginPosition, endPosition) coordinates. The coordinates may be sourced from table columns or by computing arithmetic expressions thereof. The index tracks any updates to the underlying table as usual, with one caveat explained below. Once indexed, the table can be queried for all features overlapping a query range. A GRI query yields a rowid set, which your SQL query can select from the indexed table for further filtering or analysis. Please review the brief SQLite documentation on rowid and Autoincrement . Conventions Range positions are considered zero-based & half-open , so the length of a feature is exactly endPosition-beginPosition nucleotides. The implementation doesn't strictly require this convention, but we strongly recommend observing it to minimize confusion. There is no practical limit on chromosome length, as position values may go up to 2 60 , but queries have a runtime factor logarithmic in the maximum feature length. The extension provides routines to populate a small _gri_refseq table describing the genomic reference sequences, which other tables can reference by integer ID (\"rid\") instead of storing a column with textual sequence names like 'chr10'. This convention is not required, as the GRI can index either chromosome name or rid columns, but reasons to observe it include: Integers are more compact and faster to look up. Results sort properly with ORDER BY rid instead of considering e.g. 'chr10' < 'chr2' lexicographically. A table with chromosome names can be reconstructed easily by joining with _gri_refseq . Create GRI \u21aa Create Genomic Range Index SQL: Generate a string containing a series of SQL statements which when executed create a GRI on an existing table. Executing them is left to the caller, perhaps after logging the contents. The statements should be executed within a transaction to succeed or fail atomically. Python create_gri_sql = genomicsqlite . create_genomic_range_index_sql ( dbconn , 'tableName' , 'chromosome' , 'beginPosition' , 'endPosition' ) dbconn . executescript ( create_gri_sql ) Java String griSql = GenomicSQLite . createGenomicRangeIndexSQL ( dbconn , \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); dbconn . createStatement (). executeUpdate ( griSql ); SQLiteCpp std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( create_gri_sql ); C++ std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , create_gri_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * create_genomic_range_index_sql ( const char * table , const char * rid , const char * beg , const char * end , int floor ); char * create_gri_sql = create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" , - 1 ); if ( * create_gri_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , create_gri_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */ } sqlite3_free ( create_gri_sql ); The three arguments following the table name tell the indexing procedure how to read the feature coordinates from each table row. The reference sequence may be sourced either by the name of a text column containing names like 'chr10', or of an integer reference ID (rid) column, as discussed above. The begin and end positions are read from named integer columns, or by computing simple arithmetic expressions thereof. For example, if the table happens to have beginPosition and featureLength columns, the end position may be formulated 'beginPosition+featureLength' . \u2757 The table name and expressions are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. A last optional integer argument floor can be omitted or left at -1. GRI performance may be improved slightly by setting floor to a positive integer F if the following is true: the lengths of the indexed features are almost all >16 F -1 , with only very few outlier lengths \u226416 F -1 . For example, human exons are almost all >16nt; one may therefore set floor=2 as a modest optimization for such data. YMMV The indexing script will, among other steps, add a few generated columns to the original table. So if you later SELECT * FROM tableName , you'll get these extra values back (column names starting with _gri_ ). The extra columns are \"virtual\" so they don't take up space in the table itself, but they do end up populating the stored index. At present, GRI cannot be used on WITHOUT ROWID tables. Query GRI The extension supplies a special SQL function to query a GRI-indexed table, generating the set of rowids identifying features that overlap a query range (queryChrom, queryBegin, queryEnd): genomic_range_rowids(tableName, queryChrom, queryBegin, queryEnd[, ceiling, floor]) This is typically used to retrieve the result rows by selecting for tableName._rowid_ IN genomic_range_rowids(...) . For example, SELECT col1 , col2 , ... FROM exons WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 ) The queryChrom parameter might have SQL type TEXT or INTEGER, according to whether the GRI indexes name or rid. The ordered rowid set identifies the features satisfying, queryChrom = featureChrom AND NOT ( queryBegin > featureEnd OR queryEnd < featureBegin ) ( \"query is not disjoint from feature\" ) By the half-open position convention, this includes features that abut as well as those that overlap the query range. If you don't want those, or if you want only \"contained\" features, simply add such constraints to your query's WHERE clause. The query will not match any rows with NULL feature coordinates. If needed, the GRI can inform this query for NULL chromosome/rid: SELECT ... FROM tableName WHERE _gri_rid IS NULL . Level bounds optimization The optional, trailing ceiling & floor arguments to genomic_range_rowids() optimize GRI queries by skipping steps that'd be useless in view of the length distribution of the indexed features. (See Internals for full explanation.) The extension supplies a SQL helper function genomic_range_index_levels(tableName) to detect the appropriate bounds for (the current snapshot of) the table. Example usage: SELECT col1 , col2 , ... FROM exons , genomic_range_index_levels ( 'exons' ) WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 , _gri_ceiling , _gri_floor ) Here _gri_ceiling and _gri_floor are columns of the single row computed by genomic_range_index_levels('exons') . Alternatively, your program might first query genomic_range_index_levels() alone, then pass the bounds in to subsequent prepared queries, e.g. in Python: ( gri_ceiling , gri_floor ) = next ( con . execute ( \"SELECT * FROM genomic_range_index_levels('exons')\" ) ) for ( queryChrom , queryBegin , queryEnd ) in queryRanges : exons = list ( con . execute ( \"SELECT * from exons WHERE exons._rowid_ IN \\ genomic_range_rowids('exons',?,?,?,?,?)\" , ( queryChrom , queryBegin , queryEnd , gri_ceiling , gri_floor ) ) ) ... This bounds detection procedure has a small cost, which will be worthwhile if used to optimize many subsequent GRI queries (but possibly not if just for a few). \u2757 The bounds should be redetected if the min/max feature length may have been changed by inserts or updates to the table. GRI queries with incorrect bounds are liable to produce incomplete results. Omitting the bounds is always safe, albeit slower. Instead of detecting current bounds, they can be figured manually as follows. Set the integer ceiling to C , 0 < C < 16, such that all (present & future) indexed features are guaranteed to have lengths \u226416 C . For example, if you're querying features on the human genome, then you can set ceiling=7 because the lengthiest chromosome sequence is <16 7 nt. Set the integer floor F to (i) the floor value supplied at GRI creation, if any; (ii) F > 0 such that the minimum possible feature length >16 F -1 , if any; or (iii) zero. The default, safe, albeit slower bounds are C=15, F=0. Joining tables on range overlap Suppose we have two tables with genomic features to join on range overlap. Only the \"right-hand\" table must have a GRI; preferably the smaller of the two. For example, annotating a table of variants with the surrounding exon(s), if any: SELECT variants . * , exons . _rowid_ FROM genomic_range_index_levels ( 'exons' ), variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos , _gri_ceiling , _gri_floor ) We fill out the GRI query range using the three coordinate columns of the variants table. The level bounds optimization is highly desirable for the \"tight loop\" of GRI queries during a join. See also \"Advice for big data\" below. Reference genome metadata The following routines support the aforementioned, recommended convention for storing a _gri_refseq table with information about the genomic reference sequences, which other tables can cross-reference by integer ID (rid) instead of storing textual chromosome names. The columns of _gri_refseq include: _gri_rid INTEGER PRIMARY KEY gri_refseq_name TEXT NOT NULL gri_refseq_length INTEGER NOT NULL gri_assembly TEXT genome assembly name (optional) gri_refget_id TEXT refget sequence ID (optional) gri_refseq_meta_json TEXT DEFAULT '{}' JSON object with arbitrary metadata \u21aa Put Reference Assembly SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq and populates it with information about a reference assembly whose details are bundled into the extension. Python refseq_sql = genomicsqlite . put_reference_assembly_sql ( dbconn , 'GRCh38_no_alt_analysis_set' ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceAssemblySQL ( dbconn , \"GRCh38_no_alt_analysis_set\" ); dbconn . createStatement (). executeUpdate ( refSql ); SQLiteCpp std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" )); C++ std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_assembly_sql ( const char * assembly , const char * attached_schema ); char * refseq_sql = put_genomic_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" , nullptr ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); Available assemblies: GRCh38_no_alt_analysis_set \u21aa Put Reference Sequence SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq (if it doesn't exist) and adds one reference sequence with supplied attributes. Python refseq_sql = genomicsqlite . put_reference_sequence_sql ( dbconn , 'chr17' , 83257441 # optional: assembly, refget_id, meta (dict), rid ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceSequenceSQL ( dbconn , \"chr17\" , 83257441L // optional overloads: // String assembly, String refget_id, String meta_json, long rid ); dbconn . createStatement (). executeUpdate ( refSql ); SQLiteCpp std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceSequenceSQL ( \"chr17\" , 83257441 )); C++ std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"chr17\" , 83257441 ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_sequence_sql ( const char * name , sqlite3_int64 length , const char * assembly , const char * refget_id , const char * meta_json , sqlite3_int64 rid , const char * attached_schema ); char * refseq_sql = put_genomic_reference_sequence_sql ( \"chr17\" , 83257441 , 0 , 0 , 0 , - 1 , 0 ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); If the rid argument is omitted or -1 then it will be assigned automatically upon insertion. \u21aa Get Reference Sequences by Rid: create an in-memory lookup table of the previously-stored reference information, keyed by rid integer. Assumes the stored information is read-only by this point. This table is for the application code's convenience to read tables that use the rid convention. Such uses can be also be served by SQL join on the _gri_refseq table (see Cookbook). Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_rid = genomicsqlite . get_reference_sequences_by_rid ( dbconn ) # refseq_by_rid: Dict[int, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < Long , ReferenceSequence > refseqByRid = GenomicSQLite . getReferenceSequencesByRid ( dbconn ); SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ The optional assembly argument restricts the retrieved sequences to those with matching gri_assembly value. However, mixing different assemblies in _gri_refseq is not recommended. \u21aa Get Reference Sequences by Name: create an in-memory lookup table of the previously-stored reference information, keyed by sequence name. Assumes the stored information is read-only by this point. This table is for the application code's convenience to translate name to rid whilst formulating queries or inserting features from a text source. Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_name = genomicsqlite . get_reference_sequences_by_name ( dbconn ) # refseq_by_name: Dict[str, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < String , ReferenceSequence > refseqByName = GenomicSQLite . getReferenceSequencesByName ( dbconn ); SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ Cookbook rid to chromosome name Table identifies each feature's chromosome by rid, and we want to see them with text chromosome names. SELECT gri_refseq_name , feature_table . * FROM feature_table NATURAL JOIN _gri_refseq The join key here is _gri_rid , which is one of the generated columns added by GRI creation. Alternatively, the application code can read rid from the row and translate it using the lookup table generated by the Get Reference Sequences by Rid routine. Query rid using chromosome name We're making a GRI query on a table that stores rid integers, but our query range has a chromosome name. SELECT feature_table . * FROM ( SELECT _gri_rid AS rid FROM _gri_refseq WHERE gri_refseq_name = 'chr12' ) AS query , feature_table WHERE feature_table . _rowid_ IN genomic_range_rowids ( 'feature_table' , query . rid , 111803912 , 111804012 ) We use a subquery to look up the rid corresponding to the known chromosome name. Alternatively, the application code can first convert the query name to rid using the lookup table generated by the Get Reference Sequences by Name routine. Circular chromosome query On circular chromosomes, range queries should include features that wrap around the origin to end inside the desired range. If we've stored them naively with featureEnd = featureBegin + featureLength, then we can build a unified query with reference to the stored chromosome lengths: SELECT col1 , col2 , ... FROM ( SELECT gri_refseq_length FROM _gri_refseq WHERE _gri_rid = queryRid ), featureTable WHERE featureTable . _rowid_ IN ( genomic_range_rowids ( 'featureTable' , queryRid , queryBegin , queryEnd ) UNION genomic_range_rowids ( 'featureTable' , queryRid , gri_refseq_length + queryBegin , gri_refseq_length + queryEnd )) We query a second range beyond the chromosome length, which will match features that wrap around into the query. UNION deduplicates the result rowids. As a convention, set \"circular\": true in the _gri_refseq.gri_refseq_meta_json for circular chromosomes. Advice for big data A SQLite table's rowid order indicates its physical storage layout. It's therefore preferable for a mainly-GRI-queried table to have had its rows originally inserted in genomic range order, so that the features' (chromosome, beginPosition) monotonically increase with rowid, and range queries enjoy storage/cache locality. While not required in theory, this may be needed in practice for GRI queries that will match a material fraction of a big table's rows. You can sort the rows of an existing table into a new table with the same schema, with something like INSERT INTO sorted SELECT * FROM original NOT INDEXED ORDER BY chromosome, beginPosition . The Genomics Extension enables SQLite's parallel, external merge-sorter to execute this efficiently; still, if it's feasible to load sorted data upfront, so much the better. Note 1. Make sure SQLite is configured to use a suitable storage subsystem for big temporary files generated whilst sorting. NOT INDEXED forces SQLite to use the external sorter instead of some index that'd mislead it into reading the entire table in a shuffled order. Note 2. The \"original\" table should come from a separate attached database to avoid DROP TABLE original from the final database, which is costly due to the need to defragment afterwards. A series of many GRI queries (including in the course of a join) should also proceed in genomic range order. If this isn't possible, then ideally the database page cache should be enlarged to fit the entire indexed table in memory. If you expect a GRI query to yield a very large, contiguous rowid result set (e.g. all features on a chromosome, in a table known to be range-sorted), then the following specialized query plan may be advantageous: Ask GRI for first relevant rowid, SELECT MIN(_rowid_) AS firstRowid FROM genomic_range_rowids(...) Open a cursor on SELECT ... FROM tableName WHERE _rowid_ >= firstRowid Loop through rows for as long as they're relevant. But this plan strongly depends on the contiguity assumption. Other routines Attach GenomicSQLite database \u21aa GenomicSQLite Attach: Generate a string containing a series of SQL statements to execute on an existing database connection in order to ATTACH a GenomicSQLite database under a given schema name. The main connection may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'any.db' , uri = True ) attach_sql = genomicsqlite . attach_sql ( dbconn , 'compressed.db' , 'db2' ) # attach_sql() also takes configuration keyword arguments like # genomicsqlite.connect() dbconn . executescript ( attach_sql ) # compressed.db now attached as db2 Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:any.db\" ); String attachSql = GenomicSQLite . attachSQL ( dbconn , \"compressed.db\" , \"db2\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( attachSql ); // compressed.db now attached as db2 SQLiteCpp std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); SQLite :: Database dbconn ( \"any.db\" , SQLITE_OPEN_URI ); dbconn . exec ( attach_sql ); // compressed.db now attached as db2 C++ std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , attach_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // compressed.db now attached as db2 C char * genomicsqlite_attach_sql ( const char * dbfile , const char * schema_name , const char * config_json ); char * attach_sql = genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , \"{}\" ); if ( * attach_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , attach_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( attach_sql ); /* compressed.db now attached as db2 */ \u2757 The file and schema names are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. Compress existing SQLite3 database \u21aa GenomicSQLite Vacuum Into: Generate a string containing a series of SQL statements to execute on an existing database in order to copy it into a new compressed & defragmented file. The source database may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'existing.db' , uri = True ) vacuum_sql = genomicsqlite . vacuum_into_sql ( dbconn , 'compressed.db' ) # vacuum_into_sql() also takes configuration keyword arguments like # genomicsqlite.connect() to control compression level & page sizes dbconn . executescript ( vacuum_sql ) dbconn2 = genomicsqlite . connect ( 'compressed.db' ) Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:existing.db\" ); String vacuumSql = GenomicSQLite . vacuumIntoSQL ( dbconn , \"compressed.db\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( vaccumSql ); Connection dbconn2 = DriverManager . getConnection ( \"jdbc:genomicsqlite:compressed.db\" ); SQLiteCpp std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); SQLite :: Database dbconn ( \"existing.db\" , SQLITE_OPEN_READONLY | SQLITE_OPEN_URI ); dbconn . exec ( vacuum_sql ); auto dbconn2 = GenomicSQLiteOpen ( \"compressed.db\" ); C++ std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , vacuum_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // rc = GenomicSQLiteOpen(\"compressed.db\", ...); C char * genomicsqlite_vacuum_into_sql ( const char * dest_filename , const char * config_json ); char * vacuum_sql = genomicsqlite_vacuum_into_sql ( \"compressed.db\" , \"{}\" ); if ( * vacuum_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , vacuum_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( vacuum_sql ); /* genomicsqlite_open(\"compressed.db\", ...); */ Genomics Extension version \u21aa GenomicSQLite Version SQL SELECT genomicsqlite_version () Python genomicsqlite . __version__ Java String genomicsqliteVersion = GenomicSQLite . version ( dbconn ); C++ std :: string GenomicSQLiteVersion (); C char * genomicsqlite_version (); /* result to be sqlite3_free() */ JSON functions The Genomics Extension bundles the SQLite developers' JSON1 extension and enables it automatically. The following conventions are recommended, JSON object columns should be named *_json with type TEXT DEFAULT '{}' . JSON array columns should be named *_jsarray with type TEXT DEFAULT '[]' . The JSON1 functions can be used with generated columns to effectively enable indices on JSON-embedded fields. genomicsqlite interactive shell The Python package includes a genomicsqlite script that starts the sqlite3 interactive shell with the Genomics Extension enabled. Simply invoke, $ genomicsqlite /path/to/compressed.db [-readonly] to enter the SQL prompt with the database open. Or, add an SQL statement (in quotes) to perform and exit. If you've installed the Python package but the script isn't found, you probably need to augment your PATH with the directory for Python console scripts.","title":"Programming Guide"},{"location":"guide/#programming-guide","text":"The Genomics Extension integrates with your programming language's existing SQLite3 bindings to provide a familiar experience wherever possible. Python: sqlite3 Java/JVM: sqlite-jdbc C++: SQLiteCpp (optional, recommended) or directly using... C: SQLite C/C++ API","title":"Programming Guide"},{"location":"guide/#loading-the-extension","text":"Python import sqlite3 import genomicsqlite Java import java.sql.* ; import net.mlin.genomicsqlite.GenomicSQLite ; C++ // link program to sqlite3 and genomicsqlite libraries; optionally, SQLiteCpp: // https://github.com/SRombauts/SQLiteCpp #include <sqlite3.h> #include \"SQLiteCpp/SQLiteCpp.h\" // optional #include \"genomicsqlite.h\" // General note: most GenomicSQLite C++ routines are liable to throw. C /* link program to sqlite3 and genomicsqlite libraries */ #include <sqlite3.h> #include \"genomicsqlite.h\" /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */","title":"Loading the extension"},{"location":"guide/#opening-a-compressed-database","text":"\u21aa GenomicSQLite Open: create or open a compressed database, returning a connection object with various settings pre-tuned for large datasets. Python dbconn = genomicsqlite . connect ( db_filename , read_only = False , ** kwargs # genomicsqlite + sqlite3.connect() arguments ) assert isinstance ( dbconn , sqlite3 . Connection ) Java java . util . Properties config = new java . util . Properties (); config . setProperty ( \"genomicsqlite.config_json\" , \"{}\" ); // Properties may originate from org.sqlite.SQLiteConfig.toProperties() // with genomicsqlite.config_json added in. Connection dbconn = DriverManager . getConnection ( \"jdbc:genomicsqlite:\" + dbfileName , config ); SQLiteCpp std :: unique_ptr < SQLite :: Database > GenomicSQLiteOpen ( const std :: string & db_filename , int flags = 0 , const std :: string & config_json = \"{}\" ); C++ int GenomicSQLiteOpen ( const std :: string & db_filename , sqlite3 ** ppDb , std :: string & errmsg_out , int flags = 0 , // as sqlite3_open_v2 () e . g . SQLITE_OPEN_READONLY const std :: string & config_json = \" {} \" ) noexcept ; // returns sqlite3_open_v2() code C int genomicsqlite_open ( const char * db_filename , sqlite3 ** ppDb , char ** pzErrMsg , /* if nonnull and an error occurs, set to error message * which caller should sqlite3_free() */ int flags , /* as sqlite3_open_v2() e.g. SQLITE_OPEN_READONLY */ const char * config_json /* JSON text (may be null) */ ); /* returns sqlite3_open_v2() code */ Afterwards, all the usual SQLite3 API operations are available through the returned connection object, which should finally be closed in the usual way. The storage compression layer operates transparently underneath. \u2757 GenomicSQLite databases should only be opened using this routine. If a program opens an existing GenomicSQLite database using a generic SQLite3 API, it will find a valid database whose schema is that of the compression layer instead of the intended application's. Writing into that schema might effectively corrupt the database!","title":"Opening a compressed database"},{"location":"guide/#tuning-options","text":"The aforementioned tuned settings can be further adjusted. Some bindings (e.g. C/C++) receive these options as the text of a JSON object with keys and values, while others admit individual arguments to the Open routine. threads = -1 : thread budget for compression, sort, and prefetching/decompression operations; -1 to match up to 8 host processors. Set 1 to disable all background processing. inner_page_KiB = 16 : SQLite page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. Larger pages are more compressible, but increase random I/O cost. outer_page_KiB = 32 : compression layer page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. The default configuration (inner_page_KiB, outer_page_KiB) = (16,32) balances random access speed and compression. Try setting them to (8,16) to prioritize random access, or (64,2) to prioritize compression (if compressed database will be <4TB) . zstd_level = 6 : Zstandard compression level for newly written data (-7 to 22) unsafe_load = false : set true to disable write transaction safety (see advice on bulk-loading below). \u2757 A database written to unsafely is liable to be corrupted if the application crashes, or if there's a concurrent attempt to modify it. page_cache_MiB = 1024 : database cache size. Use a large cache to avoid repeated decompression in successive and complex queries. immutable = false : set true to slightly reduce overhead reading from a database file that won't be modified by this or any concurrent program, guaranteed. force_prefetch = false : set true to enable background prefetching/decompression even if inner_page_KiB < 16 (enabled by default only \u2265 that, as it can be counterproductive below; YMMV) The connection's potential memory usage can usually be budgeted as roughly the page cache size, plus the size of any uncommitted write transaction (unless unsafe_load), plus some safety factor. \u2757However, this can multiply by (threads+1) during queries whose results are at least that large and must be re-sorted. That includes index creation, when the indexed columns total such size.","title":"Tuning options"},{"location":"guide/#advice-for-big-data","text":"Tips for writing large databases quickly: sqlite3_config(SQLITE_CONFIG_MEMSTATUS, 0) if available, to reduce overhead in SQLite3's allocation routines. Open database with unsafe_load = true to reduce transaction processing overhead (at aforementioned risk) for the connection's lifetime. Also open with the flag SQLITE_OPEN_NOMUTEX , if your application naturally serializes operations on the connection. Perform all of the following steps within one big SQLite transaction, committed at the end. Insert data rows reusing prepared, parameterized SQL statements. For tables with explicit primary keys: insert their rows in primary key order, if feasible. Consider preparing data in producer thread(s), with a consumer thread executing insertion statements in a tight loop. Bind text/blob parameters using SQLITE_STATIC if suitable. Create secondary indices, including genomic range indices, only after loading all row data. Use partial indices when they suffice. Compression guidelines The Zstandard -based compression layer is effective at capturing the typically high compressibility of bioinformatics data. But, one should expect a general-purpose database to need some extra space to keep everything organized, compared to a file format dedicated to one predetermined, read-only schema. To set a rough expectation, the maintainers feel fairly satisfied if the database file size isn't more than double that of a bespoke compression format \u2014 especially if it includes useful indices (which if well-designed, should be relatively incompressible). With SQLite's row-major table storage format , the first read of a lone cell usually entails decompressing at least its whole row, and there aren't any special column encodings for deltas, run lengths, etc. The \"last mile\" of optimization may therefore involve certain schema compromises, such as storing infrequently-accessed columns in a separate table to join when needed, or using application-layer encodings with BLOB I/O . The aforementioned zstd_level, threads, and page_size options all affect the compression time-space tradeoff, while enlarging the page cache can reduce decompression overhead (workload-dependent).","title":"Advice for big data"},{"location":"guide/#genomic-range-indexing","text":"","title":"Genomic range indexing"},{"location":"guide/#overview","text":"GenomicSQLite enables creation of a Genomic Range Index (GRI) for any database table in which each row represents a genomic feature with (chromosome, beginPosition, endPosition) coordinates. The coordinates may be sourced from table columns or by computing arithmetic expressions thereof. The index tracks any updates to the underlying table as usual, with one caveat explained below. Once indexed, the table can be queried for all features overlapping a query range. A GRI query yields a rowid set, which your SQL query can select from the indexed table for further filtering or analysis. Please review the brief SQLite documentation on rowid and Autoincrement .","title":"Overview"},{"location":"guide/#conventions","text":"Range positions are considered zero-based & half-open , so the length of a feature is exactly endPosition-beginPosition nucleotides. The implementation doesn't strictly require this convention, but we strongly recommend observing it to minimize confusion. There is no practical limit on chromosome length, as position values may go up to 2 60 , but queries have a runtime factor logarithmic in the maximum feature length. The extension provides routines to populate a small _gri_refseq table describing the genomic reference sequences, which other tables can reference by integer ID (\"rid\") instead of storing a column with textual sequence names like 'chr10'. This convention is not required, as the GRI can index either chromosome name or rid columns, but reasons to observe it include: Integers are more compact and faster to look up. Results sort properly with ORDER BY rid instead of considering e.g. 'chr10' < 'chr2' lexicographically. A table with chromosome names can be reconstructed easily by joining with _gri_refseq .","title":"Conventions"},{"location":"guide/#create-gri","text":"\u21aa Create Genomic Range Index SQL: Generate a string containing a series of SQL statements which when executed create a GRI on an existing table. Executing them is left to the caller, perhaps after logging the contents. The statements should be executed within a transaction to succeed or fail atomically. Python create_gri_sql = genomicsqlite . create_genomic_range_index_sql ( dbconn , 'tableName' , 'chromosome' , 'beginPosition' , 'endPosition' ) dbconn . executescript ( create_gri_sql ) Java String griSql = GenomicSQLite . createGenomicRangeIndexSQL ( dbconn , \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); dbconn . createStatement (). executeUpdate ( griSql ); SQLiteCpp std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( create_gri_sql ); C++ std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , create_gri_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * create_genomic_range_index_sql ( const char * table , const char * rid , const char * beg , const char * end , int floor ); char * create_gri_sql = create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" , - 1 ); if ( * create_gri_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , create_gri_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */ } sqlite3_free ( create_gri_sql ); The three arguments following the table name tell the indexing procedure how to read the feature coordinates from each table row. The reference sequence may be sourced either by the name of a text column containing names like 'chr10', or of an integer reference ID (rid) column, as discussed above. The begin and end positions are read from named integer columns, or by computing simple arithmetic expressions thereof. For example, if the table happens to have beginPosition and featureLength columns, the end position may be formulated 'beginPosition+featureLength' . \u2757 The table name and expressions are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. A last optional integer argument floor can be omitted or left at -1. GRI performance may be improved slightly by setting floor to a positive integer F if the following is true: the lengths of the indexed features are almost all >16 F -1 , with only very few outlier lengths \u226416 F -1 . For example, human exons are almost all >16nt; one may therefore set floor=2 as a modest optimization for such data. YMMV The indexing script will, among other steps, add a few generated columns to the original table. So if you later SELECT * FROM tableName , you'll get these extra values back (column names starting with _gri_ ). The extra columns are \"virtual\" so they don't take up space in the table itself, but they do end up populating the stored index. At present, GRI cannot be used on WITHOUT ROWID tables.","title":"Create GRI"},{"location":"guide/#query-gri","text":"The extension supplies a special SQL function to query a GRI-indexed table, generating the set of rowids identifying features that overlap a query range (queryChrom, queryBegin, queryEnd): genomic_range_rowids(tableName, queryChrom, queryBegin, queryEnd[, ceiling, floor]) This is typically used to retrieve the result rows by selecting for tableName._rowid_ IN genomic_range_rowids(...) . For example, SELECT col1 , col2 , ... FROM exons WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 ) The queryChrom parameter might have SQL type TEXT or INTEGER, according to whether the GRI indexes name or rid. The ordered rowid set identifies the features satisfying, queryChrom = featureChrom AND NOT ( queryBegin > featureEnd OR queryEnd < featureBegin ) ( \"query is not disjoint from feature\" ) By the half-open position convention, this includes features that abut as well as those that overlap the query range. If you don't want those, or if you want only \"contained\" features, simply add such constraints to your query's WHERE clause. The query will not match any rows with NULL feature coordinates. If needed, the GRI can inform this query for NULL chromosome/rid: SELECT ... FROM tableName WHERE _gri_rid IS NULL .","title":"Query GRI"},{"location":"guide/#level-bounds-optimization","text":"The optional, trailing ceiling & floor arguments to genomic_range_rowids() optimize GRI queries by skipping steps that'd be useless in view of the length distribution of the indexed features. (See Internals for full explanation.) The extension supplies a SQL helper function genomic_range_index_levels(tableName) to detect the appropriate bounds for (the current snapshot of) the table. Example usage: SELECT col1 , col2 , ... FROM exons , genomic_range_index_levels ( 'exons' ) WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 , _gri_ceiling , _gri_floor ) Here _gri_ceiling and _gri_floor are columns of the single row computed by genomic_range_index_levels('exons') . Alternatively, your program might first query genomic_range_index_levels() alone, then pass the bounds in to subsequent prepared queries, e.g. in Python: ( gri_ceiling , gri_floor ) = next ( con . execute ( \"SELECT * FROM genomic_range_index_levels('exons')\" ) ) for ( queryChrom , queryBegin , queryEnd ) in queryRanges : exons = list ( con . execute ( \"SELECT * from exons WHERE exons._rowid_ IN \\ genomic_range_rowids('exons',?,?,?,?,?)\" , ( queryChrom , queryBegin , queryEnd , gri_ceiling , gri_floor ) ) ) ... This bounds detection procedure has a small cost, which will be worthwhile if used to optimize many subsequent GRI queries (but possibly not if just for a few). \u2757 The bounds should be redetected if the min/max feature length may have been changed by inserts or updates to the table. GRI queries with incorrect bounds are liable to produce incomplete results. Omitting the bounds is always safe, albeit slower. Instead of detecting current bounds, they can be figured manually as follows. Set the integer ceiling to C , 0 < C < 16, such that all (present & future) indexed features are guaranteed to have lengths \u226416 C . For example, if you're querying features on the human genome, then you can set ceiling=7 because the lengthiest chromosome sequence is <16 7 nt. Set the integer floor F to (i) the floor value supplied at GRI creation, if any; (ii) F > 0 such that the minimum possible feature length >16 F -1 , if any; or (iii) zero. The default, safe, albeit slower bounds are C=15, F=0.","title":"Level bounds optimization"},{"location":"guide/#joining-tables-on-range-overlap","text":"Suppose we have two tables with genomic features to join on range overlap. Only the \"right-hand\" table must have a GRI; preferably the smaller of the two. For example, annotating a table of variants with the surrounding exon(s), if any: SELECT variants . * , exons . _rowid_ FROM genomic_range_index_levels ( 'exons' ), variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos , _gri_ceiling , _gri_floor ) We fill out the GRI query range using the three coordinate columns of the variants table. The level bounds optimization is highly desirable for the \"tight loop\" of GRI queries during a join. See also \"Advice for big data\" below.","title":"Joining tables on range overlap"},{"location":"guide/#reference-genome-metadata","text":"The following routines support the aforementioned, recommended convention for storing a _gri_refseq table with information about the genomic reference sequences, which other tables can cross-reference by integer ID (rid) instead of storing textual chromosome names. The columns of _gri_refseq include: _gri_rid INTEGER PRIMARY KEY gri_refseq_name TEXT NOT NULL gri_refseq_length INTEGER NOT NULL gri_assembly TEXT genome assembly name (optional) gri_refget_id TEXT refget sequence ID (optional) gri_refseq_meta_json TEXT DEFAULT '{}' JSON object with arbitrary metadata \u21aa Put Reference Assembly SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq and populates it with information about a reference assembly whose details are bundled into the extension. Python refseq_sql = genomicsqlite . put_reference_assembly_sql ( dbconn , 'GRCh38_no_alt_analysis_set' ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceAssemblySQL ( dbconn , \"GRCh38_no_alt_analysis_set\" ); dbconn . createStatement (). executeUpdate ( refSql ); SQLiteCpp std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" )); C++ std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_assembly_sql ( const char * assembly , const char * attached_schema ); char * refseq_sql = put_genomic_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" , nullptr ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); Available assemblies: GRCh38_no_alt_analysis_set \u21aa Put Reference Sequence SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq (if it doesn't exist) and adds one reference sequence with supplied attributes. Python refseq_sql = genomicsqlite . put_reference_sequence_sql ( dbconn , 'chr17' , 83257441 # optional: assembly, refget_id, meta (dict), rid ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceSequenceSQL ( dbconn , \"chr17\" , 83257441L // optional overloads: // String assembly, String refget_id, String meta_json, long rid ); dbconn . createStatement (). executeUpdate ( refSql ); SQLiteCpp std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceSequenceSQL ( \"chr17\" , 83257441 )); C++ std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"chr17\" , 83257441 ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_sequence_sql ( const char * name , sqlite3_int64 length , const char * assembly , const char * refget_id , const char * meta_json , sqlite3_int64 rid , const char * attached_schema ); char * refseq_sql = put_genomic_reference_sequence_sql ( \"chr17\" , 83257441 , 0 , 0 , 0 , - 1 , 0 ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); If the rid argument is omitted or -1 then it will be assigned automatically upon insertion. \u21aa Get Reference Sequences by Rid: create an in-memory lookup table of the previously-stored reference information, keyed by rid integer. Assumes the stored information is read-only by this point. This table is for the application code's convenience to read tables that use the rid convention. Such uses can be also be served by SQL join on the _gri_refseq table (see Cookbook). Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_rid = genomicsqlite . get_reference_sequences_by_rid ( dbconn ) # refseq_by_rid: Dict[int, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < Long , ReferenceSequence > refseqByRid = GenomicSQLite . getReferenceSequencesByRid ( dbconn ); SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ The optional assembly argument restricts the retrieved sequences to those with matching gri_assembly value. However, mixing different assemblies in _gri_refseq is not recommended. \u21aa Get Reference Sequences by Name: create an in-memory lookup table of the previously-stored reference information, keyed by sequence name. Assumes the stored information is read-only by this point. This table is for the application code's convenience to translate name to rid whilst formulating queries or inserting features from a text source. Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_name = genomicsqlite . get_reference_sequences_by_name ( dbconn ) # refseq_by_name: Dict[str, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < String , ReferenceSequence > refseqByName = GenomicSQLite . getReferenceSequencesByName ( dbconn ); SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */","title":"Reference genome metadata"},{"location":"guide/#cookbook","text":"","title":"Cookbook"},{"location":"guide/#rid-to-chromosome-name","text":"Table identifies each feature's chromosome by rid, and we want to see them with text chromosome names. SELECT gri_refseq_name , feature_table . * FROM feature_table NATURAL JOIN _gri_refseq The join key here is _gri_rid , which is one of the generated columns added by GRI creation. Alternatively, the application code can read rid from the row and translate it using the lookup table generated by the Get Reference Sequences by Rid routine.","title":"rid to chromosome name"},{"location":"guide/#query-rid-using-chromosome-name","text":"We're making a GRI query on a table that stores rid integers, but our query range has a chromosome name. SELECT feature_table . * FROM ( SELECT _gri_rid AS rid FROM _gri_refseq WHERE gri_refseq_name = 'chr12' ) AS query , feature_table WHERE feature_table . _rowid_ IN genomic_range_rowids ( 'feature_table' , query . rid , 111803912 , 111804012 ) We use a subquery to look up the rid corresponding to the known chromosome name. Alternatively, the application code can first convert the query name to rid using the lookup table generated by the Get Reference Sequences by Name routine.","title":"Query rid using chromosome name"},{"location":"guide/#circular-chromosome-query","text":"On circular chromosomes, range queries should include features that wrap around the origin to end inside the desired range. If we've stored them naively with featureEnd = featureBegin + featureLength, then we can build a unified query with reference to the stored chromosome lengths: SELECT col1 , col2 , ... FROM ( SELECT gri_refseq_length FROM _gri_refseq WHERE _gri_rid = queryRid ), featureTable WHERE featureTable . _rowid_ IN ( genomic_range_rowids ( 'featureTable' , queryRid , queryBegin , queryEnd ) UNION genomic_range_rowids ( 'featureTable' , queryRid , gri_refseq_length + queryBegin , gri_refseq_length + queryEnd )) We query a second range beyond the chromosome length, which will match features that wrap around into the query. UNION deduplicates the result rowids. As a convention, set \"circular\": true in the _gri_refseq.gri_refseq_meta_json for circular chromosomes.","title":"Circular chromosome query"},{"location":"guide/#advice-for-big-data_1","text":"A SQLite table's rowid order indicates its physical storage layout. It's therefore preferable for a mainly-GRI-queried table to have had its rows originally inserted in genomic range order, so that the features' (chromosome, beginPosition) monotonically increase with rowid, and range queries enjoy storage/cache locality. While not required in theory, this may be needed in practice for GRI queries that will match a material fraction of a big table's rows. You can sort the rows of an existing table into a new table with the same schema, with something like INSERT INTO sorted SELECT * FROM original NOT INDEXED ORDER BY chromosome, beginPosition . The Genomics Extension enables SQLite's parallel, external merge-sorter to execute this efficiently; still, if it's feasible to load sorted data upfront, so much the better. Note 1. Make sure SQLite is configured to use a suitable storage subsystem for big temporary files generated whilst sorting. NOT INDEXED forces SQLite to use the external sorter instead of some index that'd mislead it into reading the entire table in a shuffled order. Note 2. The \"original\" table should come from a separate attached database to avoid DROP TABLE original from the final database, which is costly due to the need to defragment afterwards. A series of many GRI queries (including in the course of a join) should also proceed in genomic range order. If this isn't possible, then ideally the database page cache should be enlarged to fit the entire indexed table in memory. If you expect a GRI query to yield a very large, contiguous rowid result set (e.g. all features on a chromosome, in a table known to be range-sorted), then the following specialized query plan may be advantageous: Ask GRI for first relevant rowid, SELECT MIN(_rowid_) AS firstRowid FROM genomic_range_rowids(...) Open a cursor on SELECT ... FROM tableName WHERE _rowid_ >= firstRowid Loop through rows for as long as they're relevant. But this plan strongly depends on the contiguity assumption.","title":"Advice for big data"},{"location":"guide/#other-routines","text":"","title":"Other routines"},{"location":"guide/#attach-genomicsqlite-database","text":"\u21aa GenomicSQLite Attach: Generate a string containing a series of SQL statements to execute on an existing database connection in order to ATTACH a GenomicSQLite database under a given schema name. The main connection may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'any.db' , uri = True ) attach_sql = genomicsqlite . attach_sql ( dbconn , 'compressed.db' , 'db2' ) # attach_sql() also takes configuration keyword arguments like # genomicsqlite.connect() dbconn . executescript ( attach_sql ) # compressed.db now attached as db2 Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:any.db\" ); String attachSql = GenomicSQLite . attachSQL ( dbconn , \"compressed.db\" , \"db2\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( attachSql ); // compressed.db now attached as db2 SQLiteCpp std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); SQLite :: Database dbconn ( \"any.db\" , SQLITE_OPEN_URI ); dbconn . exec ( attach_sql ); // compressed.db now attached as db2 C++ std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , attach_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // compressed.db now attached as db2 C char * genomicsqlite_attach_sql ( const char * dbfile , const char * schema_name , const char * config_json ); char * attach_sql = genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , \"{}\" ); if ( * attach_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , attach_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( attach_sql ); /* compressed.db now attached as db2 */ \u2757 The file and schema names are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input.","title":"Attach GenomicSQLite database"},{"location":"guide/#compress-existing-sqlite3-database","text":"\u21aa GenomicSQLite Vacuum Into: Generate a string containing a series of SQL statements to execute on an existing database in order to copy it into a new compressed & defragmented file. The source database may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'existing.db' , uri = True ) vacuum_sql = genomicsqlite . vacuum_into_sql ( dbconn , 'compressed.db' ) # vacuum_into_sql() also takes configuration keyword arguments like # genomicsqlite.connect() to control compression level & page sizes dbconn . executescript ( vacuum_sql ) dbconn2 = genomicsqlite . connect ( 'compressed.db' ) Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:existing.db\" ); String vacuumSql = GenomicSQLite . vacuumIntoSQL ( dbconn , \"compressed.db\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( vaccumSql ); Connection dbconn2 = DriverManager . getConnection ( \"jdbc:genomicsqlite:compressed.db\" ); SQLiteCpp std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); SQLite :: Database dbconn ( \"existing.db\" , SQLITE_OPEN_READONLY | SQLITE_OPEN_URI ); dbconn . exec ( vacuum_sql ); auto dbconn2 = GenomicSQLiteOpen ( \"compressed.db\" ); C++ std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , vacuum_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // rc = GenomicSQLiteOpen(\"compressed.db\", ...); C char * genomicsqlite_vacuum_into_sql ( const char * dest_filename , const char * config_json ); char * vacuum_sql = genomicsqlite_vacuum_into_sql ( \"compressed.db\" , \"{}\" ); if ( * vacuum_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , vacuum_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( vacuum_sql ); /* genomicsqlite_open(\"compressed.db\", ...); */","title":"Compress existing SQLite3 database"},{"location":"guide/#genomics-extension-version","text":"\u21aa GenomicSQLite Version SQL SELECT genomicsqlite_version () Python genomicsqlite . __version__ Java String genomicsqliteVersion = GenomicSQLite . version ( dbconn ); C++ std :: string GenomicSQLiteVersion (); C char * genomicsqlite_version (); /* result to be sqlite3_free() */","title":"Genomics Extension version"},{"location":"guide/#json-functions","text":"The Genomics Extension bundles the SQLite developers' JSON1 extension and enables it automatically. The following conventions are recommended, JSON object columns should be named *_json with type TEXT DEFAULT '{}' . JSON array columns should be named *_jsarray with type TEXT DEFAULT '[]' . The JSON1 functions can be used with generated columns to effectively enable indices on JSON-embedded fields.","title":"JSON functions"},{"location":"guide/#genomicsqlite-interactive-shell","text":"The Python package includes a genomicsqlite script that starts the sqlite3 interactive shell with the Genomics Extension enabled. Simply invoke, $ genomicsqlite /path/to/compressed.db [-readonly] to enter the SQL prompt with the database open. Or, add an SQL statement (in quotes) to perform and exit. If you've installed the Python package but the script isn't found, you probably need to augment your PATH with the directory for Python console scripts.","title":"genomicsqlite interactive shell"},{"location":"internals/","text":"Genomics Extension Internals Compression layer To be written; for now see sqlite_zstd_vfs Genomic Range Index The GenomicSQLite GRI is a conventional multi-column B-tree index, organized so that feature overlap with any length distribution is detectable using a series of SQL \"between tuples\" queries. Partition the features into \"levels\" according to their length. Each level L for 0 \u2264 L < 16 consists of all features with 16 L-1 < length \u2264 16 L . (Level 0 has features of length 0 or 1.) The GRI is a multi-column SQL index on each feature's: (chrom, level, position, length) The features on level L overlapping a query range (qchrom, qpos, qend) are: ((chrom, level, position) BETWEEN (qchrom, L , qbeg - 16 L ) AND (qchrom, L , qend)) AND position+length \u2265 qbeg. SQLite understands this query more-or-less as shown, and plans efficiently to (i) search the index B-tree for the first entry whose tuple (chrom, level, position) \u2265 (qchrom, L , qbeg - 16 L ), (ii) scan while it's \u2264 (qchrom, L , qend), and (iii) apply the last filter. We search higher levels over exponentially wider position ranges, but they only harbor features of comparable length. Union the disjoint results for L between 0 and 15. We can optimize queries to \"fan out\" to fewer than 16 levels (which gratuitously admit lengths up to 2 60 nt) by first figuring out the lowest and highest level actually occupied by the indexed features (floor and ceiling). This can be learned during amortized query planning, using a few quick B-tree searches per chromosome in the existing index ( genomic_range_index_levels() ). Or, the caller can supply looser bounds based on lengthiest chromosome or other prior knowledge. The fan-out is only 3 or 4 in many practical datasets. Sometimes it's inflated by a few outlier short features, which e.g. make the bottommost occupied level 0 or 1 instead of 2 or 3. In that case, we can simply push the short outliers up to a higher level \u2014 that's the floor argument to Create Genomic Range Index SQL . One last detail: we negate the level number L in the B-tree index, so that when it's built up in genomic range order, smaller (typically more-numerous) features tend to insert into the rightmost leaf \u2014 that's usually faster than interior insertions. This scheme combines the main ideas from Ensembl's query based on maximum feature length with the multi-level binning used in UCSC Genome Browser , BAI , tabix , and bedtools . It addresses the former's sensitivity to outlier lengthy features. And it's simpler than the latter, without effective constraints on feature/chromosome length. A downside is that the index takes more space, storing four values per feature. Feature length could be omitted from the B-tree index to save space at the cost of query speed. Including it lets our index query determine the exact result set without accessing the main table at all. chrom and level could be consolidated into one integer with some loss of flexibility. SQLite storage uses a variable-length integer encoding anyway. Example query Here's a GRI query on levels 0-2 using statement parameters (?1, ?2, ?3) = (queryChrom, queryBeg, queryEnd). ( SELECT _rowid_ FROM ( SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 2 ,( ? 2 ) - 0 x100 ) AND (( ? 1 ), - 2 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 1 ,( ? 2 ) - 0 x10 ) AND (( ? 1 ), - 1 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 0 ,( ? 2 ) - 0 x1 ) AND (( ? 1 ), - 0 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 )) ORDER BY _rowid_ ) The _gri_* columns are virtual generated columns added to the table based on the feature coordinate expressions specified during GRI creation. The no-op subtraction of 0 from qend was found to be needed for SQLite to use the intended query plan in all cases. So too with the repetition of the length filter constraint, which looks like it should be factored out to the outer SELECT. \ud83e\udd37 Generating subquery SQL The genomic_range_rowids() SQL function implementation just executes a query like the above and streams results back to the caller, taking care to minimize overhead by reusing prepared SQLite statements. The extension also exposes the routine to generate the SQL subquery, which you can textually paste into your SQL in place of the genomic_range_rowids() invocation. This direct method is more efficient, given proper use of prepared statements, because everything compiles into one pure SQLite bytecode program , instead of context-switching into the extension for each result. \u21aa Genomic Range Rowids SQL : Generate a string containing a parenthesized SELECT query on a GRI-indexed table, which when executed yields a rowid result set identifying the overlapping features. This is typically pasted as a subquery within a larger query that retrieves the result rows for further filtering/analysis. Python query = ( 'SELECT * FROM tableName WHERE tableName._rowid_ IN ' + genomicsqlite . genomic_range_rowids_sql ( dbconn , 'tableName' , # defaults: qrid = '?1' , qbeg = '?2' , qend = '?3' , ceiling =- 1 , floor =- 1 ) ) cursor = dbconn . execute ( query , ( 'chr12' , 111803912 , 111804012 )) SQLiteCpp std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // SQLite::Database* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn -> getHandle ()); SQLite :: Statement stmt ( * dbconn , query ); stmt . bindNoCopy ( 1 , \"chr12\" ); stmt . bind ( 2 , ( sqlite3_int64 ) 111803912 ); stmt . bind ( 3 , ( sqlite3_int64 ) 111804012 ); while ( stmt . executeStep ()) { // process row } C++ std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // sqlite3* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn ); // Omitted for brevity: // Compile query using sqlite3_prepare_v3() // Bind query range parameters using sqlite3_bind_{text,int64}() // Step through results as usual with sqlite3_step() C char * genomic_range_rowids_sql ( const char * indexed_table , sqlite3 * dbconn , const char * qrid , /* null defaults to \"?1\" */ const char * qbeg , /* null defaults to \"?2\" */ const char * qend , /* null defaults to \"?3\" */ int ceiling , int floor /* set these to -1, not 0! */ ); /* sqlite3* dbconn */ char * subquery = genomic_range_rowids_sql ( \"tableName\" , dbconn , 0 , 0 , 0 , - 1 , - 1 ); if ( * subquery ) { /* Omitted for brevity: * Append subquery to \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" * Compile query using sqlite3_prepare_v3() * Bind query range parameters using sqlite3_bind_{text,int64}() * Step through results as usual with sqlite3_step() */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( subquery ); Following the name of the indexed table to be queried , the routine takes three arguments supplying the desired range to query it for (queryChrom, queryBegin, queryEnd). These arguments default to ?1 , ?2 , and ?3 , sourcing the first three bound parameters of the top-level SQL query. They can be overridden to: other numbered or named parameter placeholders literal SQL values names of columns in other tables being joined simple expressions involving any of the above \u2757 The table name and expressions are textually pasted into a SQL template. Take care to prevent SQL injection, if they're in any way determined by external input. Unlike genomic_range_rowids() , the genomic_range_rowids_sql subquery generator defaults to level bounds auto-detected from the table at the time of subquery generation. \u2757 If the generated SQL query will be reused on a table that may have changed in the meantime, then ceiling & floor should be overidden based on the maximum and minimum possible feature length (ceiling=15 and floor=0 for full generality). If your language bindings don't include a wrapper for this routine, you can use the database connection to get the text result of SELECT genomic_range_rowids_sql(tableName[, qrid, qbeg, qend[, ceiling, floor]]) .","title":"Internals"},{"location":"internals/#genomics-extension-internals","text":"","title":"Genomics Extension Internals"},{"location":"internals/#compression-layer","text":"To be written; for now see sqlite_zstd_vfs","title":"Compression layer"},{"location":"internals/#genomic-range-index","text":"The GenomicSQLite GRI is a conventional multi-column B-tree index, organized so that feature overlap with any length distribution is detectable using a series of SQL \"between tuples\" queries. Partition the features into \"levels\" according to their length. Each level L for 0 \u2264 L < 16 consists of all features with 16 L-1 < length \u2264 16 L . (Level 0 has features of length 0 or 1.) The GRI is a multi-column SQL index on each feature's: (chrom, level, position, length) The features on level L overlapping a query range (qchrom, qpos, qend) are: ((chrom, level, position) BETWEEN (qchrom, L , qbeg - 16 L ) AND (qchrom, L , qend)) AND position+length \u2265 qbeg. SQLite understands this query more-or-less as shown, and plans efficiently to (i) search the index B-tree for the first entry whose tuple (chrom, level, position) \u2265 (qchrom, L , qbeg - 16 L ), (ii) scan while it's \u2264 (qchrom, L , qend), and (iii) apply the last filter. We search higher levels over exponentially wider position ranges, but they only harbor features of comparable length. Union the disjoint results for L between 0 and 15. We can optimize queries to \"fan out\" to fewer than 16 levels (which gratuitously admit lengths up to 2 60 nt) by first figuring out the lowest and highest level actually occupied by the indexed features (floor and ceiling). This can be learned during amortized query planning, using a few quick B-tree searches per chromosome in the existing index ( genomic_range_index_levels() ). Or, the caller can supply looser bounds based on lengthiest chromosome or other prior knowledge. The fan-out is only 3 or 4 in many practical datasets. Sometimes it's inflated by a few outlier short features, which e.g. make the bottommost occupied level 0 or 1 instead of 2 or 3. In that case, we can simply push the short outliers up to a higher level \u2014 that's the floor argument to Create Genomic Range Index SQL . One last detail: we negate the level number L in the B-tree index, so that when it's built up in genomic range order, smaller (typically more-numerous) features tend to insert into the rightmost leaf \u2014 that's usually faster than interior insertions. This scheme combines the main ideas from Ensembl's query based on maximum feature length with the multi-level binning used in UCSC Genome Browser , BAI , tabix , and bedtools . It addresses the former's sensitivity to outlier lengthy features. And it's simpler than the latter, without effective constraints on feature/chromosome length. A downside is that the index takes more space, storing four values per feature. Feature length could be omitted from the B-tree index to save space at the cost of query speed. Including it lets our index query determine the exact result set without accessing the main table at all. chrom and level could be consolidated into one integer with some loss of flexibility. SQLite storage uses a variable-length integer encoding anyway.","title":"Genomic Range Index"},{"location":"internals/#example-query","text":"Here's a GRI query on levels 0-2 using statement parameters (?1, ?2, ?3) = (queryChrom, queryBeg, queryEnd). ( SELECT _rowid_ FROM ( SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 2 ,( ? 2 ) - 0 x100 ) AND (( ? 1 ), - 2 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 1 ,( ? 2 ) - 0 x10 ) AND (( ? 1 ), - 1 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 0 ,( ? 2 ) - 0 x1 ) AND (( ? 1 ), - 0 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 )) ORDER BY _rowid_ ) The _gri_* columns are virtual generated columns added to the table based on the feature coordinate expressions specified during GRI creation. The no-op subtraction of 0 from qend was found to be needed for SQLite to use the intended query plan in all cases. So too with the repetition of the length filter constraint, which looks like it should be factored out to the outer SELECT. \ud83e\udd37","title":"Example query"},{"location":"internals/#generating-subquery-sql","text":"The genomic_range_rowids() SQL function implementation just executes a query like the above and streams results back to the caller, taking care to minimize overhead by reusing prepared SQLite statements. The extension also exposes the routine to generate the SQL subquery, which you can textually paste into your SQL in place of the genomic_range_rowids() invocation. This direct method is more efficient, given proper use of prepared statements, because everything compiles into one pure SQLite bytecode program , instead of context-switching into the extension for each result. \u21aa Genomic Range Rowids SQL : Generate a string containing a parenthesized SELECT query on a GRI-indexed table, which when executed yields a rowid result set identifying the overlapping features. This is typically pasted as a subquery within a larger query that retrieves the result rows for further filtering/analysis. Python query = ( 'SELECT * FROM tableName WHERE tableName._rowid_ IN ' + genomicsqlite . genomic_range_rowids_sql ( dbconn , 'tableName' , # defaults: qrid = '?1' , qbeg = '?2' , qend = '?3' , ceiling =- 1 , floor =- 1 ) ) cursor = dbconn . execute ( query , ( 'chr12' , 111803912 , 111804012 )) SQLiteCpp std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // SQLite::Database* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn -> getHandle ()); SQLite :: Statement stmt ( * dbconn , query ); stmt . bindNoCopy ( 1 , \"chr12\" ); stmt . bind ( 2 , ( sqlite3_int64 ) 111803912 ); stmt . bind ( 3 , ( sqlite3_int64 ) 111804012 ); while ( stmt . executeStep ()) { // process row } C++ std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // sqlite3* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn ); // Omitted for brevity: // Compile query using sqlite3_prepare_v3() // Bind query range parameters using sqlite3_bind_{text,int64}() // Step through results as usual with sqlite3_step() C char * genomic_range_rowids_sql ( const char * indexed_table , sqlite3 * dbconn , const char * qrid , /* null defaults to \"?1\" */ const char * qbeg , /* null defaults to \"?2\" */ const char * qend , /* null defaults to \"?3\" */ int ceiling , int floor /* set these to -1, not 0! */ ); /* sqlite3* dbconn */ char * subquery = genomic_range_rowids_sql ( \"tableName\" , dbconn , 0 , 0 , 0 , - 1 , - 1 ); if ( * subquery ) { /* Omitted for brevity: * Append subquery to \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" * Compile query using sqlite3_prepare_v3() * Bind query range parameters using sqlite3_bind_{text,int64}() * Step through results as usual with sqlite3_step() */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( subquery ); Following the name of the indexed table to be queried , the routine takes three arguments supplying the desired range to query it for (queryChrom, queryBegin, queryEnd). These arguments default to ?1 , ?2 , and ?3 , sourcing the first three bound parameters of the top-level SQL query. They can be overridden to: other numbered or named parameter placeholders literal SQL values names of columns in other tables being joined simple expressions involving any of the above \u2757 The table name and expressions are textually pasted into a SQL template. Take care to prevent SQL injection, if they're in any way determined by external input. Unlike genomic_range_rowids() , the genomic_range_rowids_sql subquery generator defaults to level bounds auto-detected from the table at the time of subquery generation. \u2757 If the generated SQL query will be reused on a table that may have changed in the meantime, then ceiling & floor should be overidden based on the maximum and minimum possible feature length (ceiling=15 and floor=0 for full generality). If your language bindings don't include a wrapper for this routine, you can use the database connection to get the text result of SELECT genomic_range_rowids_sql(tableName[, qrid, qbeg, qend[, ceiling, floor]]) .","title":"Generating subquery SQL"}]}