{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Genomics Extension for SQLite (\"GenomicSQLite\") https://github.com/mlin/GenomicSQLite This SQLite3 loadable extension supports applications in genome bioinformatics by adding: genomic range indexing for overlap queries & joins in-SQL utility functions, e.g. reverse-complement DNA, parse \"chr1:2,345-6,789\" automatic streaming storage compression reading directly from HTTP(S) URLs pre-tuned settings for \"big data\" This October 2020 poster discusses the context and long-run ambitions: Our Colab notebook demonstrates key features with Python, one of several language bindings. USE AT YOUR OWN RISK: The extension makes fundamental changes to the database storage layer. While designed to preserve ACID transaction safety, it's young and unlikely to have zero bugs. This project is not associated with the SQLite developers. SQLite \u2265 3.31.0 required To use the Genomics Extension you might first need to upgrade SQLite itself. The host program must link SQLite version 3.31.0 (2020-01-22) or newer. In your shell, sqlite3 --version displays the version installed with your OS, which is probably what your programs use; if in doubt, cause a program to report the result of SELECT sqlite_version() . If this is too old, then upgrade SQLite3 using your preferred binary package manager (e.g. apt, yum, conda, brew), if possible. Otherwise, modify your program's linking step or runtime environment to cause it to use an up-to-date version, for example by setting rpath or LD_LIBRARY_PATH to the location of an up-to-date shared library file. Resources: How To Compile SQLite DreamHost Knowledge Base - Installing a custom version of SQLite3 Rpmfind: libsqlite3 Sqlite :: Anaconda Cloud Homebrew formula/sqlite , formula-linux/sqlite As a last resort for GNU/Linux, our GitHub Releases include a libsqlite3.so.0 that should be compatible with modern (2015+) hosts. You can always SELECT sqlite_version() to verify the upgrade in your program. Installation It's usually easiest to obtain the extension as a pre-compiled shared library (Linux .so or macOS .dylib), either installed with a package manager, or downloaded from GitHub Releases . Python pip3 install [ --user | --system ] genomicsqlite # -or- conda install -c mlin genomicsqlite The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. JVM Add entries like the following to your Maven pom.xml : <repositories> <repository> <id> genomicsqlite-jdbc </id> <url> https://raw.githubusercontent.com/wiki/mlin/GenomicSQLite/mvn-repo/ </url> </repository> </repositories> <dependencies> <dependency> <groupId> net.mlin </groupId> <artifactId> genomicsqlite-jdbc </artifactId> <version> vX.Y.Z </version> </dependency> </dependencies> Or, download the JAR from GitHub Releases and place it in your classpath, along with sqlite-jdbc 's JAR which is also required. The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. Rust Add to your project's Cargo.toml : [dependencies.genomicsqlite] version = \"^0\" Building with the crate will include a platform-appropriate shared library file within your compilation unit, to be extracted & loaded at runtime. To disable this, add default-features = false and at runtime, set environment variable LIBGENOMICSQLITE to a filename or place the library file somewhere it'll be found by dlopen(\"libgenomicsqlite\") . C/C++ Download zip of shared library and genomicsqlite.h from GitHub Releases . Build your program with them, and also ensure the dynamic linker will find the shared library at runtime, by either: (1) installing it in a system or user lib directory & refreshing cache, (2) setting LD_LIBRARY_PATH environment variable, (3) building with -rpath . Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. GNU/Linux: to link the prebuilt libgenomicsqlite.so distributed from our GitHub Releases, you may have to compile your source with CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 . This is because the library is built against an old libstdc++ version to improve runtime compatibility. The function of this flag is explained in the libstdc++ docs on Dual ABI . If you build libgenomicsqlite.so from source, then the flag will not be needed. See our GitHub README for the source build procedure, if needed. We welcome community contributions to the available language bindings; see the Language Bindings Guide if interested. Smoke test We recommend trying our \"smoke test\" script to quickly verify your local system's compatibility. It requires the Python package installed per the instructions above. wget -nv -O - https://raw.githubusercontent.com/mlin/GenomicSQLite/release/test/genomicsqlite_smoke_test.py \\ | python3 - Even if you're not planning to use Python, this test's detailed logs may be useful to diagnose general problems loading the extension. Use cases The extension makes SQLite an efficient foundation for: Integrative genomics data warehouse BED, GFF/GTF, FASTA, FASTQ, SAM, VCF, ... One file, zero administration, portable between platforms and languages Slicing & basic analysis with indexed SQL queries, joins, & aggregations Transactional storage engine for API services, incremental reanalysis, real-time basecalling & metagenomics, ... Experimental new data models, before dedicated storage format & tooling are warranted, if ever. Contraindications Huge numerical arrays: see HDF5 , Zarr , Parquet , Arrow , TileDB . SQLite's BLOB I/O leaves the door open for mash-ups! Parallel SQL analytics / OLAP: see Spark , DuckDB , many commercial products. (Some bases can be covered with a sharding harness for a pool of threads with their own SQLite connections...) Streaming: SQLite storage relies on randomly reading & writing throughout the database file. Proceed to the Programming Guide section to get started building applications.","title":"Intro & Install"},{"location":"#genomics-extension-for-sqlite","text":"","title":"Genomics Extension for SQLite"},{"location":"#genomicsqlite","text":"https://github.com/mlin/GenomicSQLite This SQLite3 loadable extension supports applications in genome bioinformatics by adding: genomic range indexing for overlap queries & joins in-SQL utility functions, e.g. reverse-complement DNA, parse \"chr1:2,345-6,789\" automatic streaming storage compression reading directly from HTTP(S) URLs pre-tuned settings for \"big data\" This October 2020 poster discusses the context and long-run ambitions: Our Colab notebook demonstrates key features with Python, one of several language bindings. USE AT YOUR OWN RISK: The extension makes fundamental changes to the database storage layer. While designed to preserve ACID transaction safety, it's young and unlikely to have zero bugs. This project is not associated with the SQLite developers.","title":"(\"GenomicSQLite\")"},{"location":"#sqlite-3310-required","text":"To use the Genomics Extension you might first need to upgrade SQLite itself. The host program must link SQLite version 3.31.0 (2020-01-22) or newer. In your shell, sqlite3 --version displays the version installed with your OS, which is probably what your programs use; if in doubt, cause a program to report the result of SELECT sqlite_version() . If this is too old, then upgrade SQLite3 using your preferred binary package manager (e.g. apt, yum, conda, brew), if possible. Otherwise, modify your program's linking step or runtime environment to cause it to use an up-to-date version, for example by setting rpath or LD_LIBRARY_PATH to the location of an up-to-date shared library file. Resources: How To Compile SQLite DreamHost Knowledge Base - Installing a custom version of SQLite3 Rpmfind: libsqlite3 Sqlite :: Anaconda Cloud Homebrew formula/sqlite , formula-linux/sqlite As a last resort for GNU/Linux, our GitHub Releases include a libsqlite3.so.0 that should be compatible with modern (2015+) hosts. You can always SELECT sqlite_version() to verify the upgrade in your program.","title":"SQLite &ge; 3.31.0 required"},{"location":"#installation","text":"It's usually easiest to obtain the extension as a pre-compiled shared library (Linux .so or macOS .dylib), either installed with a package manager, or downloaded from GitHub Releases . Python pip3 install [ --user | --system ] genomicsqlite # -or- conda install -c mlin genomicsqlite The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. JVM Add entries like the following to your Maven pom.xml : <repositories> <repository> <id> genomicsqlite-jdbc </id> <url> https://raw.githubusercontent.com/wiki/mlin/GenomicSQLite/mvn-repo/ </url> </repository> </repositories> <dependencies> <dependency> <groupId> net.mlin </groupId> <artifactId> genomicsqlite-jdbc </artifactId> <version> vX.Y.Z </version> </dependency> </dependencies> Or, download the JAR from GitHub Releases and place it in your classpath, along with sqlite-jdbc 's JAR which is also required. The package loads a bundled shared library by default. To override the bundled file, set environment variable LIBGENOMICSQLITE to a filename. Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. Rust Add to your project's Cargo.toml : [dependencies.genomicsqlite] version = \"^0\" Building with the crate will include a platform-appropriate shared library file within your compilation unit, to be extracted & loaded at runtime. To disable this, add default-features = false and at runtime, set environment variable LIBGENOMICSQLITE to a filename or place the library file somewhere it'll be found by dlopen(\"libgenomicsqlite\") . C/C++ Download zip of shared library and genomicsqlite.h from GitHub Releases . Build your program with them, and also ensure the dynamic linker will find the shared library at runtime, by either: (1) installing it in a system or user lib directory & refreshing cache, (2) setting LD_LIBRARY_PATH environment variable, (3) building with -rpath . Recommendation: also install the Python package, which includes a useful command-line shell and smoke-test script. GNU/Linux: to link the prebuilt libgenomicsqlite.so distributed from our GitHub Releases, you may have to compile your source with CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 . This is because the library is built against an old libstdc++ version to improve runtime compatibility. The function of this flag is explained in the libstdc++ docs on Dual ABI . If you build libgenomicsqlite.so from source, then the flag will not be needed. See our GitHub README for the source build procedure, if needed. We welcome community contributions to the available language bindings; see the Language Bindings Guide if interested.","title":"Installation"},{"location":"#smoke-test","text":"We recommend trying our \"smoke test\" script to quickly verify your local system's compatibility. It requires the Python package installed per the instructions above. wget -nv -O - https://raw.githubusercontent.com/mlin/GenomicSQLite/release/test/genomicsqlite_smoke_test.py \\ | python3 - Even if you're not planning to use Python, this test's detailed logs may be useful to diagnose general problems loading the extension.","title":"Smoke test"},{"location":"#use-cases","text":"The extension makes SQLite an efficient foundation for: Integrative genomics data warehouse BED, GFF/GTF, FASTA, FASTQ, SAM, VCF, ... One file, zero administration, portable between platforms and languages Slicing & basic analysis with indexed SQL queries, joins, & aggregations Transactional storage engine for API services, incremental reanalysis, real-time basecalling & metagenomics, ... Experimental new data models, before dedicated storage format & tooling are warranted, if ever.","title":"Use cases"},{"location":"#contraindications","text":"Huge numerical arrays: see HDF5 , Zarr , Parquet , Arrow , TileDB . SQLite's BLOB I/O leaves the door open for mash-ups! Parallel SQL analytics / OLAP: see Spark , DuckDB , many commercial products. (Some bases can be covered with a sharding harness for a pool of threads with their own SQLite connections...) Streaming: SQLite storage relies on randomly reading & writing throughout the database file. Proceed to the Programming Guide section to get started building applications.","title":"Contraindications"},{"location":"bindings/","text":"Writing Language Bindings Thank you for considering a contribution to the language bindings available for the Genomics Extension! Overview The extension is a C++11 library, and C/C++ programs compile against it in the usual way to invoke the routines outlined in the Programming Guide. But for other programming languages, another option should make it unnecessary to use a C/C++ foreign-function interface . The key routines are also exposed as custom SQL functions , which can be invoked by SELECT statements on any SQLite connection, once the extension has been loaded. New language bindings consist largely of ~one-liner functions that pass arguments through to SELECT routine(arg1,arg2,...) and return the result, usually a TEXT value. None are performance-sensitive, as long as developers use prepared, parameterized SQLite3 statements for GRI queries in loops. Bindings should endeavor to integrate \"naturally\" with the host language and its existing SQLite3 bindings. For example, the object returned by the Open procedure should be an idiomatic SQLite3 connection object. Also, APIs should follow the host language's conventions for naming style and optional/keyword arguments. Our Python module can be followed as an illustrative example. Locating & loading the extension library The module should first locate the extension shared-library file (e.g. libgenomicsqlite.so on Linux), but it doesn't actually load it; instead, it tells SQLite to load it . This can occur either during global module initialization or on the first connection attempt. Select the shared-library file in the following order of preference. For development, start with prebuilt binaries from GitHub Releases . Value of LIBGENOMICSQLITE environment variable, if nonempty Platform-appropriate file shipped with the bindings, if you choose to do so (see Packaging, below) Fall back to \"libgenomicsqlite\" to let SQLite use dlopen() to look for it in default locations Use the language SQLite3 bindings to open a connection to a :memory: database , which will just serve these initialization operations. On the connection, enable extension loading if needed and perform sqlite3_load_extension() , or equivalent, on the library filename. The extension needs to be loaded only once per process: upon first loading, it registers itself to activate automatically on each new connection opened. GenomicSQLite Open The Open method should \"look like\" the language's existing wrapper around sqlite3_open() , taking similar arguments and returning the same type of connection object. It uses two routines from the GenomicSQLite library, exposed as SQL functions, which help with activating the compression layer and tuning various settings. Given a database filename, the method follows these steps: 1. Generate connection string with SELECT genomicsqlite_uri(dbfilename, config_json) This helper generates a text URI based on the given filename, which tells SQLite how to open it with the compression layer activated. Call it on your :memory: connection. config_json is the text of a JSON object containing keys and values for the several tuning options shown in the Programming Guide. Any supplied settings are merged into a hard-coded default JSON, so it's only necessary to specify values that need to be overridden, and fine to pass '{}' if there are none. The Open method should allow the caller to supply these optional arguments in some linguistically natural way, then take care of formulating the JSON text. You can access the hard-coded defaults with SELECT genomicsqlite_default_config_json() , which may be useful to determine the available keys. (For example, the Python bindings use this to distinguish the GenomicSQLite options from other optional arguments meant to be passed through to SQLite.) 2. Call sqlite3_open() using the connection string Give the URI connection string to the normal SQLite open method. You must set the SQLITE_OPEN_URI flag or equivalent for this to work. The Open method should pass other flags based on optional arguments in the same way as the existing SQLite3 wrapper. If the caller requested a read-only connection, Open can either set SQLITE_OPEN_READONLY or append &mode=ro to the connection string. 3. Generate tuning script with SELECT genomicsqlite_tuning_sql(config_json) Through the new connection, pass the same config_json described above to this helper. The helper doesn't actually do anything to the database; it merely generates text of an SQL script (semicolon-separated statements) for you to execute. 4. Execute tuning script on the new connection The existing SQLite3 bindings probably expose some method to imperatively execute the semicolon-separated SQL script in one shot. 5. Return the connection object At this point the connection is ready to go, and it should not be necessary to wrap it. Other routines The other routines are much simpler. The binding for each just takes its required and optional arguments, passes them through to a SELECT routine(...) statement on the caller-supplied connection object, and returns the single text answer. SELECT genomicsqlite_version() SELECT genomicsqlite_attach_sql(dbfilename, schema_name, config_json) takes the same config_json as would Open SELECT genomicsqlite_vacuum_into_sql(destfilename, config_json) SELECT create_genomic_range_index_sql(tableName, chromosome, beginPosition, endPosition[, floor]) : floor is an integer, others text. SELECT put_genomic_reference_assembly_sql(assembly[, schema]) SELECT put_genomic_reference_sequence_sql(name, length[, assembly, refget_id, meta_json, rid, schema]) length and rid are integers. Optional text arguments can default to NULL, and optional integers can default to -1. The bindings for Get Reference Sequences by Rid just read the _gri_refseq table like, SELECT _gri_rid, gri_refseq_name, gri_refseq_length, gri_assembly, gri_refget_id, gri_refseq_meta_json FROM [schema.]_gri_refseq [WHERE assembly = ?] and loads the results into some linguistically-natural data structure that'll provide quick lookup of those attributes by rid. Then, Get Reference Sequences by Name can simply call that and \"invert\" the results. Packaging Our GitHub Releases supply prebuilt extension binaries intended to be compatible with most modern hosts. There are at least three packaging options: Bundle nothing: require the end user to download the right library file (or build it themselves) and place it where the bindings and SQLite3 will be able to find them, as described above. Bundle our binaries: ship our binaries inside your package and choose the right one to load at runtime. Remember, the only actual ABI linking occurs between the extension library and SQLite3 itself, so there's nothing to worry about compatibility between the library and the host language runtime. Source build: You can specify the CMake-based source build procedure & its dependencies described in the GitHub README however the host language's packaging system does it. Documentation The MkDocs source Markdown for this documentation site is in the GitHub repository . Fork it & send us a pull request, adding appropriate examples to all the code tabs in the Installation & Programming Guide.","title":"Writing Language Bindings"},{"location":"bindings/#writing-language-bindings","text":"Thank you for considering a contribution to the language bindings available for the Genomics Extension!","title":"Writing Language Bindings"},{"location":"bindings/#overview","text":"The extension is a C++11 library, and C/C++ programs compile against it in the usual way to invoke the routines outlined in the Programming Guide. But for other programming languages, another option should make it unnecessary to use a C/C++ foreign-function interface . The key routines are also exposed as custom SQL functions , which can be invoked by SELECT statements on any SQLite connection, once the extension has been loaded. New language bindings consist largely of ~one-liner functions that pass arguments through to SELECT routine(arg1,arg2,...) and return the result, usually a TEXT value. None are performance-sensitive, as long as developers use prepared, parameterized SQLite3 statements for GRI queries in loops. Bindings should endeavor to integrate \"naturally\" with the host language and its existing SQLite3 bindings. For example, the object returned by the Open procedure should be an idiomatic SQLite3 connection object. Also, APIs should follow the host language's conventions for naming style and optional/keyword arguments. Our Python module can be followed as an illustrative example.","title":"Overview"},{"location":"bindings/#locating-loading-the-extension-library","text":"The module should first locate the extension shared-library file (e.g. libgenomicsqlite.so on Linux), but it doesn't actually load it; instead, it tells SQLite to load it . This can occur either during global module initialization or on the first connection attempt. Select the shared-library file in the following order of preference. For development, start with prebuilt binaries from GitHub Releases . Value of LIBGENOMICSQLITE environment variable, if nonempty Platform-appropriate file shipped with the bindings, if you choose to do so (see Packaging, below) Fall back to \"libgenomicsqlite\" to let SQLite use dlopen() to look for it in default locations Use the language SQLite3 bindings to open a connection to a :memory: database , which will just serve these initialization operations. On the connection, enable extension loading if needed and perform sqlite3_load_extension() , or equivalent, on the library filename. The extension needs to be loaded only once per process: upon first loading, it registers itself to activate automatically on each new connection opened.","title":"Locating &amp; loading the extension library"},{"location":"bindings/#genomicsqlite-open","text":"The Open method should \"look like\" the language's existing wrapper around sqlite3_open() , taking similar arguments and returning the same type of connection object. It uses two routines from the GenomicSQLite library, exposed as SQL functions, which help with activating the compression layer and tuning various settings. Given a database filename, the method follows these steps:","title":"GenomicSQLite Open"},{"location":"bindings/#1-generate-connection-string-with-select-genomicsqlite_uridbfilename-config_json","text":"This helper generates a text URI based on the given filename, which tells SQLite how to open it with the compression layer activated. Call it on your :memory: connection. config_json is the text of a JSON object containing keys and values for the several tuning options shown in the Programming Guide. Any supplied settings are merged into a hard-coded default JSON, so it's only necessary to specify values that need to be overridden, and fine to pass '{}' if there are none. The Open method should allow the caller to supply these optional arguments in some linguistically natural way, then take care of formulating the JSON text. You can access the hard-coded defaults with SELECT genomicsqlite_default_config_json() , which may be useful to determine the available keys. (For example, the Python bindings use this to distinguish the GenomicSQLite options from other optional arguments meant to be passed through to SQLite.)","title":"1. Generate connection string with SELECT genomicsqlite_uri(dbfilename, config_json)"},{"location":"bindings/#2-call-sqlite3_open-using-the-connection-string","text":"Give the URI connection string to the normal SQLite open method. You must set the SQLITE_OPEN_URI flag or equivalent for this to work. The Open method should pass other flags based on optional arguments in the same way as the existing SQLite3 wrapper. If the caller requested a read-only connection, Open can either set SQLITE_OPEN_READONLY or append &mode=ro to the connection string.","title":"2. Call sqlite3_open() using the connection string"},{"location":"bindings/#3-generate-tuning-script-with-select-genomicsqlite_tuning_sqlconfig_json","text":"Through the new connection, pass the same config_json described above to this helper. The helper doesn't actually do anything to the database; it merely generates text of an SQL script (semicolon-separated statements) for you to execute.","title":"3. Generate tuning script with SELECT genomicsqlite_tuning_sql(config_json)"},{"location":"bindings/#4-execute-tuning-script-on-the-new-connection","text":"The existing SQLite3 bindings probably expose some method to imperatively execute the semicolon-separated SQL script in one shot.","title":"4. Execute tuning script on the new connection"},{"location":"bindings/#5-return-the-connection-object","text":"At this point the connection is ready to go, and it should not be necessary to wrap it.","title":"5. Return the connection object"},{"location":"bindings/#other-routines","text":"The other routines are much simpler. The binding for each just takes its required and optional arguments, passes them through to a SELECT routine(...) statement on the caller-supplied connection object, and returns the single text answer. SELECT genomicsqlite_version() SELECT genomicsqlite_attach_sql(dbfilename, schema_name, config_json) takes the same config_json as would Open SELECT genomicsqlite_vacuum_into_sql(destfilename, config_json) SELECT create_genomic_range_index_sql(tableName, chromosome, beginPosition, endPosition[, floor]) : floor is an integer, others text. SELECT put_genomic_reference_assembly_sql(assembly[, schema]) SELECT put_genomic_reference_sequence_sql(name, length[, assembly, refget_id, meta_json, rid, schema]) length and rid are integers. Optional text arguments can default to NULL, and optional integers can default to -1. The bindings for Get Reference Sequences by Rid just read the _gri_refseq table like, SELECT _gri_rid, gri_refseq_name, gri_refseq_length, gri_assembly, gri_refget_id, gri_refseq_meta_json FROM [schema.]_gri_refseq [WHERE assembly = ?] and loads the results into some linguistically-natural data structure that'll provide quick lookup of those attributes by rid. Then, Get Reference Sequences by Name can simply call that and \"invert\" the results.","title":"Other routines"},{"location":"bindings/#packaging","text":"Our GitHub Releases supply prebuilt extension binaries intended to be compatible with most modern hosts. There are at least three packaging options: Bundle nothing: require the end user to download the right library file (or build it themselves) and place it where the bindings and SQLite3 will be able to find them, as described above. Bundle our binaries: ship our binaries inside your package and choose the right one to load at runtime. Remember, the only actual ABI linking occurs between the extension library and SQLite3 itself, so there's nothing to worry about compatibility between the library and the host language runtime. Source build: You can specify the CMake-based source build procedure & its dependencies described in the GitHub README however the host language's packaging system does it.","title":"Packaging"},{"location":"bindings/#documentation","text":"The MkDocs source Markdown for this documentation site is in the GitHub repository . Fork it & send us a pull request, adding appropriate examples to all the code tabs in the Installation & Programming Guide.","title":"Documentation"},{"location":"guide_db/","text":"Programming Guide - Opening Compressed Databases The Genomics Extension integrates with your programming language's existing SQLite3 bindings to provide a familiar experience wherever possible. Python: sqlite3 Java/JVM: sqlite-jdbc Rust: rusqlite C++: SQLiteCpp (optional, recommended) or directly using... C: SQLite C/C++ API First complete the installation instructions . Loading the extension Python import sqlite3 import genomicsqlite Java import java.sql.* ; import net.mlin.genomicsqlite.GenomicSQLite ; Rust use genomicsqlite :: ConnectionMethods ; use rusqlite :: { Connection , OpenFlags , params , NO_PARAMS }; The genomicsqlite::ConnectionMethods trait makes available GenomicSQLite-specific methods for rusqlite::Connection (and rusqlite::Transaction ). See rustdoc for some extra details. C++ #include <sqlite3.h> #include \"SQLiteCpp/SQLiteCpp.h\" // optional #include \"genomicsqlite.h\" int main () { try { GENOMICSQLITE_CXX_INIT (); } catch ( std :: runtime_error & exn ) { // report exn.what() } ... } Link the program to sqlite3 and genomicsqlite libraries. Optionally, include SQLiteCpp headers before genomicsqlite.h to use its more-convenient API; but don't link it, as the genomicsqlite library has it built-in. GNU/Linux: to link the prebuilt libgenomicsqlite.so distributed from our GitHub Releases, you may have to compile your source with CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 . This is because the library is built against an old libstdc++ version to improve runtime compatibility. The function of this flag is explained in the libstdc++ docs on Dual ABI . If you build libgenomicsqlite.so from source, then the flag will not be needed. General note: GenomicSQLite C++ routines are liable to throw exceptions. C #include <sqlite3.h> #include \"genomicsqlite.h\" int main () { char * zErrMsg = 0 ; int rc = GENOMICSQLITE_C_INIT ( & zErrMsg ); if ( rc != SQLITE_OK ) { /* report zErrMsg */ sqlite3_free ( zErrMsg ); } ... } Link the program to sqlite3 and genomicsqlite libraries. All GenomicSQLite C routines returning a char* string use the following convention. If the operation succeeds, then it's a nonempty, null-terminated string. Otherwise, it points to a null byte followed immediately by a nonempty, null-terminated error message. In either case, the caller must free the string with sqlite3_free() . NULL is returned only if out of memory. Opening a compressed database \u21aa GenomicSQLite Open: create or open a compressed database, returning a connection object with various settings pre-tuned for large datasets. Python dbconn = genomicsqlite . connect ( db_filename , read_only = False , ** kwargs # genomicsqlite + sqlite3.connect() arguments ) assert isinstance ( dbconn , sqlite3 . Connection ) Java java . util . Properties config = new java . util . Properties (); config . setProperty ( \"genomicsqlite.config_json\" , \"{}\" ); // Properties may originate from org.sqlite.SQLiteConfig.toProperties() // with genomicsqlite.config_json added in. Connection dbconn = DriverManager . getConnection ( \"jdbc:genomicsqlite:\" + dbfileName , config ); Rust let dbconn : Connection = genomicsqlite :: open ( db_filename , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & json :: object :: Object :: new () // tuning options ) ? ; SQLiteCpp std :: unique_ptr < SQLite :: Database > GenomicSQLiteOpen ( const std :: string & db_filename , int flags = 0 , const std :: string & config_json = \"{}\" ); C++ int GenomicSQLiteOpen ( const std :: string & db_filename , sqlite3 ** ppDb , std :: string & errmsg_out , int flags = 0 , // as sqlite3_open_v2 () e . g . SQLITE_OPEN_READONLY const std :: string & config_json = \" {} \" ) noexcept ; // returns sqlite3_open_v2() code C int genomicsqlite_open ( const char * db_filename , sqlite3 ** ppDb , char ** pzErrMsg , /* if nonnull and an error occurs, set to error message * which caller should sqlite3_free() */ int flags , /* as sqlite3_open_v2() e.g. SQLITE_OPEN_READONLY */ const char * config_json /* JSON text (may be null) */ ); /* returns sqlite3_open_v2() code */ Afterwards, all the usual SQLite3 API operations are available through the returned connection object, which should finally be closed in the usual way. The storage compression layer operates transparently underneath. \u2757 GenomicSQLite databases should only be opened using this routine. If a program opens an existing GenomicSQLite database using a generic SQLite3 API, it will find a valid database whose schema is that of the compression layer instead of the intended application's. Writing into that schema might effectively corrupt the database! Tuning options The aforementioned tuned settings can be further adjusted. Some bindings (e.g. C/C++) receive these options as the text of a JSON object with keys and values, while others admit individual arguments to the Open routine. threads = -1 : thread budget for compression, sort, and prefetching/decompression operations; -1 to match up to 8 host processors. Set 1 to disable all background processing. inner_page_KiB = 16 : SQLite page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. Larger pages are more compressible, but increase random I/O cost. outer_page_KiB = 32 : compression layer page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. The default configuration (inner_page_KiB, outer_page_KiB) = (16,32) balances random access speed and compression. Try setting them to (8,16) to prioritize random access, or (64,2) to prioritize compression (if compressed database will be <4TB) . zstd_level = 6 : Zstandard compression level for newly written data (-7 to 22) unsafe_load = false : set true to disable write transaction safety (see advice on bulk-loading below). \u2757 A database written to unsafely is liable to be corrupted if the application crashes, or if there's a concurrent attempt to modify it. page_cache_MiB = 1024 : database cache size. Use a large cache to avoid repeated decompression in successive and complex queries. immutable = false : set true to slightly reduce overhead reading from a database file that won't be modified by this or any concurrent program, guaranteed. force_prefetch = false : set true to enable background prefetching/decompression even if inner_page_KiB < 16 (enabled by default only \u2265 that, as it can be counterproductive below; YMMV) The connection's potential memory usage can usually be budgeted as roughly the page cache size, plus the size of any uncommitted write transaction (unless unsafe_load), plus some safety factor. \u2757However, this can multiply by (threads+1) during queries whose results are at least that large and must be re-sorted. That includes index creation, when the indexed columns total such size. genomicsqlite interactive shell The Python package includes a genomicsqlite script that enters the sqlite3 interactive shell on an existing compressed database. This is a convenient way to inspect and explore the data with ad hoc SQL queries, as one might use grep or awk on text files. With the Python package installed ( pip3 install genomicsqlite or conda install -c mlin genomicsqlite ): $ genomicsqlite DB_FILENAME [--readonly] to enter the SQL prompt with the database open. Or, add an SQL statement (in quotes) to perform and exit. If you've installed the Python package but the script isn't found, set your PATH to include the bin directory with Python console scripts. Database compaction. The utility has a subcommand to compress and defragment an existing database file (compressed or uncompressed), which can increase its compression level and optimize access to it. $ genomicsqlite DB_FILENAME --compact generates DB_FILENAME.compact ; see its --help for additional options, in particular --level , --inner-page-KiB and --outer-page-KiB affect the output file size as discussed above. Due to decompression overhead, the compaction procedure may be impractically slow if the database has big tables that weren't initially written in their primary key order. To prevent this, see below Optimizing storage layout . Reading databases over the web The GenomicSQLite Open routine and the genomicsqlite shell also accept http: and https: URLs instead of local filenames, creating a connection to read the compressed file over the web directly. The database connection must be opened read-only in the appropriate manner for your language bindings (such as the flag SQLITE_OPEN_READONLY ). The URL server must support HTTP GET range requests, and the content must not change for the lifetime of the connection. Under the hood, the extension uses libcurl to send web requests for necessary portions of the database file as queries proceed, with adaptive batching & prefetching to balance the number and size of these requests. This works well for point lookups and queries that scan largely-contiguous slices of tables and indexes (and a modest number thereof). It's less suitable for big multi-way joins and other aggressively random access patterns; in such cases, it'd be better to download the database file upfront to open locally. Reading large databases over the web, budget an additional ~600MiB of memory for HTTP prefetch buffers. The HTTP driver writes log messages to standard error when requests fail or had to be retried, which can be disabled by setting configuration web_log = 0 or environment SQLITE_WEB_LOG=0; or increased up to 5 to log every request and other details. To disable TLS certificate and hostname verification, set web_insecure = true in the GenomicSQLite configuration, or SQLITE_WEB_INSECURE=1 in the environment. The above-described genomicsqlite DB_FILENAME --compact optimizes a database for web access by making the request pattern more contiguous. Web access optimization with .dbi helper files Experimental feature Optionally, web access can be further optimized by a small .dbi helper file served alongside the main database file. The client automatically probes for this by appending .dbi to the database URL (unless there's a query string). If that's not usable for any reason, the database falls back to direct access. Increase the web_log to 3 or higher to see which mode is used. Use genomicsqlite DB_FILENAME --dbi to generate the .dbi helper for an immutable database file, then publish them alongside each other. The .dbi must be regenerated if the database subsequently changes. To override the automatic probe, set configuration web_dbi_url to a different URL for the .dbi file, or to a local file:/path/to.dbi downloaded beforehand. Use the latter feature to save multiple connections from each having to fetch the .dbi separately. Lastly, set web_nodbi to true or environment SQLITE_WEB_NODBI=1 to disable dbi mode entirely. The .dbi helper is optional, but often beneficial for big databases accessed with high-latency requests. It collects bits of the main file that are key for navigating it, but typically scattered throughout (even after compaction). Prefetching them in the compact .dbi saves the reader from having to pluck them from all over the main file. Advice for big data Writing large databases quickly sqlite3_config(SQLITE_CONFIG_MEMSTATUS, 0) if available, to reduce overhead in SQLite3's allocation routines. Open database with unsafe_load = true to reduce transaction processing overhead (at aforementioned risk) for the connection's lifetime. Also open with the flag SQLITE_OPEN_NOMUTEX , if your application naturally serializes operations on the connection. Perform all of the following steps within one big SQLite transaction, committed at the end. Insert data rows reusing prepared, parameterized SQL statements. Process the rows in primary key order, if feasible (otherwise, see below Optimizing storage layout ). Consider preparing data in producer thread(s), with a consumer thread executing insertion statements in a tight loop. Bind text/blob parameters using SQLITE_STATIC if suitable. Create secondary indexes, including genomic range indexes, only after loading all row data. Use partial indexes when they suffice. Optimizing storage layout For multiple reasons mentioned so far, large tables should have their rows initially inserted in primary key order (or whatever order will promote access locality), ensuring they'll be stored as such in the database file; and tables should be written one-at-a-time. If it's inconvenient to process the input data in this way, the following procedure can help: Create temporary table(s) with the same schema as the destination table(s), but omitting any PRIMARY KEY specifiers, UNIQUE constraints, or other indexes. Stream all the data into these temporary tables, which are fast to write and read, in whatever order is convenient. INSERT INTO permanent_table SELECT * FROM temp_table ORDER BY colA, colB, ... using the primary key (or other desired sort order) for each table. The Genomics Extension automatically enables SQLite's parallel, external merge-sorter to execute the last step efficiently. Ensure it's configured to use a suitable storage subsystem for big temporary files. Compression guidelines The Zstandard -based compression layer is effective at capturing the high compressibility of bioinformatics data. But, one should expect a general-purpose database to use extra space to keep everything organized, compared to a file format dedicated to one read-only schema. To set a rough expectation, the maintainers feel fairly satisfied if the database file size isn't more than double that of a bespoke compression format \u2014 especially if it includes useful indexes (which if well-designed, should be relatively incompressible). The aforementioned zstd_level, threads, and page_size options all affect the compression time-space tradeoff, while enlarging the page cache can reduce decompression overhead (workload-dependent). If you plan to delete or overwrite a significant amount of data in an existing database, issue PRAGMA secure_delete=ON beforehand to keep the compressed file as small as possible. This works by causing SQLite to overwrite unused database pages with all zeroes, which the compression layer can then reduce to a negligible size. With SQLite's row-major table storage format , the first read of a lone cell usually entails decompressing at least its whole row, and there aren't any special column encodings for deltas, run lengths, etc. The \"last mile\" of optimization may therefore involve certain schema compromises, such as storing infrequently-accessed columns in a separate table to join when needed, or using application-layer encodings with BLOB I/O .","title":"Opening Compressed Databases"},{"location":"guide_db/#programming-guide-opening-compressed-databases","text":"The Genomics Extension integrates with your programming language's existing SQLite3 bindings to provide a familiar experience wherever possible. Python: sqlite3 Java/JVM: sqlite-jdbc Rust: rusqlite C++: SQLiteCpp (optional, recommended) or directly using... C: SQLite C/C++ API First complete the installation instructions .","title":"Programming Guide - Opening Compressed Databases"},{"location":"guide_db/#loading-the-extension","text":"Python import sqlite3 import genomicsqlite Java import java.sql.* ; import net.mlin.genomicsqlite.GenomicSQLite ; Rust use genomicsqlite :: ConnectionMethods ; use rusqlite :: { Connection , OpenFlags , params , NO_PARAMS }; The genomicsqlite::ConnectionMethods trait makes available GenomicSQLite-specific methods for rusqlite::Connection (and rusqlite::Transaction ). See rustdoc for some extra details. C++ #include <sqlite3.h> #include \"SQLiteCpp/SQLiteCpp.h\" // optional #include \"genomicsqlite.h\" int main () { try { GENOMICSQLITE_CXX_INIT (); } catch ( std :: runtime_error & exn ) { // report exn.what() } ... } Link the program to sqlite3 and genomicsqlite libraries. Optionally, include SQLiteCpp headers before genomicsqlite.h to use its more-convenient API; but don't link it, as the genomicsqlite library has it built-in. GNU/Linux: to link the prebuilt libgenomicsqlite.so distributed from our GitHub Releases, you may have to compile your source with CXXFLAGS=-D_GLIBCXX_USE_CXX11_ABI=0 . This is because the library is built against an old libstdc++ version to improve runtime compatibility. The function of this flag is explained in the libstdc++ docs on Dual ABI . If you build libgenomicsqlite.so from source, then the flag will not be needed. General note: GenomicSQLite C++ routines are liable to throw exceptions. C #include <sqlite3.h> #include \"genomicsqlite.h\" int main () { char * zErrMsg = 0 ; int rc = GENOMICSQLITE_C_INIT ( & zErrMsg ); if ( rc != SQLITE_OK ) { /* report zErrMsg */ sqlite3_free ( zErrMsg ); } ... } Link the program to sqlite3 and genomicsqlite libraries. All GenomicSQLite C routines returning a char* string use the following convention. If the operation succeeds, then it's a nonempty, null-terminated string. Otherwise, it points to a null byte followed immediately by a nonempty, null-terminated error message. In either case, the caller must free the string with sqlite3_free() . NULL is returned only if out of memory.","title":"Loading the extension"},{"location":"guide_db/#opening-a-compressed-database","text":"\u21aa GenomicSQLite Open: create or open a compressed database, returning a connection object with various settings pre-tuned for large datasets. Python dbconn = genomicsqlite . connect ( db_filename , read_only = False , ** kwargs # genomicsqlite + sqlite3.connect() arguments ) assert isinstance ( dbconn , sqlite3 . Connection ) Java java . util . Properties config = new java . util . Properties (); config . setProperty ( \"genomicsqlite.config_json\" , \"{}\" ); // Properties may originate from org.sqlite.SQLiteConfig.toProperties() // with genomicsqlite.config_json added in. Connection dbconn = DriverManager . getConnection ( \"jdbc:genomicsqlite:\" + dbfileName , config ); Rust let dbconn : Connection = genomicsqlite :: open ( db_filename , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & json :: object :: Object :: new () // tuning options ) ? ; SQLiteCpp std :: unique_ptr < SQLite :: Database > GenomicSQLiteOpen ( const std :: string & db_filename , int flags = 0 , const std :: string & config_json = \"{}\" ); C++ int GenomicSQLiteOpen ( const std :: string & db_filename , sqlite3 ** ppDb , std :: string & errmsg_out , int flags = 0 , // as sqlite3_open_v2 () e . g . SQLITE_OPEN_READONLY const std :: string & config_json = \" {} \" ) noexcept ; // returns sqlite3_open_v2() code C int genomicsqlite_open ( const char * db_filename , sqlite3 ** ppDb , char ** pzErrMsg , /* if nonnull and an error occurs, set to error message * which caller should sqlite3_free() */ int flags , /* as sqlite3_open_v2() e.g. SQLITE_OPEN_READONLY */ const char * config_json /* JSON text (may be null) */ ); /* returns sqlite3_open_v2() code */ Afterwards, all the usual SQLite3 API operations are available through the returned connection object, which should finally be closed in the usual way. The storage compression layer operates transparently underneath. \u2757 GenomicSQLite databases should only be opened using this routine. If a program opens an existing GenomicSQLite database using a generic SQLite3 API, it will find a valid database whose schema is that of the compression layer instead of the intended application's. Writing into that schema might effectively corrupt the database!","title":"Opening a compressed database"},{"location":"guide_db/#tuning-options","text":"The aforementioned tuned settings can be further adjusted. Some bindings (e.g. C/C++) receive these options as the text of a JSON object with keys and values, while others admit individual arguments to the Open routine. threads = -1 : thread budget for compression, sort, and prefetching/decompression operations; -1 to match up to 8 host processors. Set 1 to disable all background processing. inner_page_KiB = 16 : SQLite page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. Larger pages are more compressible, but increase random I/O cost. outer_page_KiB = 32 : compression layer page size for new databases, any of {1, 2, 4, 8, 16, 32, 64}. The default configuration (inner_page_KiB, outer_page_KiB) = (16,32) balances random access speed and compression. Try setting them to (8,16) to prioritize random access, or (64,2) to prioritize compression (if compressed database will be <4TB) . zstd_level = 6 : Zstandard compression level for newly written data (-7 to 22) unsafe_load = false : set true to disable write transaction safety (see advice on bulk-loading below). \u2757 A database written to unsafely is liable to be corrupted if the application crashes, or if there's a concurrent attempt to modify it. page_cache_MiB = 1024 : database cache size. Use a large cache to avoid repeated decompression in successive and complex queries. immutable = false : set true to slightly reduce overhead reading from a database file that won't be modified by this or any concurrent program, guaranteed. force_prefetch = false : set true to enable background prefetching/decompression even if inner_page_KiB < 16 (enabled by default only \u2265 that, as it can be counterproductive below; YMMV) The connection's potential memory usage can usually be budgeted as roughly the page cache size, plus the size of any uncommitted write transaction (unless unsafe_load), plus some safety factor. \u2757However, this can multiply by (threads+1) during queries whose results are at least that large and must be re-sorted. That includes index creation, when the indexed columns total such size.","title":"Tuning options"},{"location":"guide_db/#genomicsqlite-interactive-shell","text":"The Python package includes a genomicsqlite script that enters the sqlite3 interactive shell on an existing compressed database. This is a convenient way to inspect and explore the data with ad hoc SQL queries, as one might use grep or awk on text files. With the Python package installed ( pip3 install genomicsqlite or conda install -c mlin genomicsqlite ): $ genomicsqlite DB_FILENAME [--readonly] to enter the SQL prompt with the database open. Or, add an SQL statement (in quotes) to perform and exit. If you've installed the Python package but the script isn't found, set your PATH to include the bin directory with Python console scripts. Database compaction. The utility has a subcommand to compress and defragment an existing database file (compressed or uncompressed), which can increase its compression level and optimize access to it. $ genomicsqlite DB_FILENAME --compact generates DB_FILENAME.compact ; see its --help for additional options, in particular --level , --inner-page-KiB and --outer-page-KiB affect the output file size as discussed above. Due to decompression overhead, the compaction procedure may be impractically slow if the database has big tables that weren't initially written in their primary key order. To prevent this, see below Optimizing storage layout .","title":"genomicsqlite interactive shell"},{"location":"guide_db/#reading-databases-over-the-web","text":"The GenomicSQLite Open routine and the genomicsqlite shell also accept http: and https: URLs instead of local filenames, creating a connection to read the compressed file over the web directly. The database connection must be opened read-only in the appropriate manner for your language bindings (such as the flag SQLITE_OPEN_READONLY ). The URL server must support HTTP GET range requests, and the content must not change for the lifetime of the connection. Under the hood, the extension uses libcurl to send web requests for necessary portions of the database file as queries proceed, with adaptive batching & prefetching to balance the number and size of these requests. This works well for point lookups and queries that scan largely-contiguous slices of tables and indexes (and a modest number thereof). It's less suitable for big multi-way joins and other aggressively random access patterns; in such cases, it'd be better to download the database file upfront to open locally. Reading large databases over the web, budget an additional ~600MiB of memory for HTTP prefetch buffers. The HTTP driver writes log messages to standard error when requests fail or had to be retried, which can be disabled by setting configuration web_log = 0 or environment SQLITE_WEB_LOG=0; or increased up to 5 to log every request and other details. To disable TLS certificate and hostname verification, set web_insecure = true in the GenomicSQLite configuration, or SQLITE_WEB_INSECURE=1 in the environment. The above-described genomicsqlite DB_FILENAME --compact optimizes a database for web access by making the request pattern more contiguous.","title":"Reading databases over the web"},{"location":"guide_db/#web-access-optimization-with-dbi-helper-files","text":"Experimental feature Optionally, web access can be further optimized by a small .dbi helper file served alongside the main database file. The client automatically probes for this by appending .dbi to the database URL (unless there's a query string). If that's not usable for any reason, the database falls back to direct access. Increase the web_log to 3 or higher to see which mode is used. Use genomicsqlite DB_FILENAME --dbi to generate the .dbi helper for an immutable database file, then publish them alongside each other. The .dbi must be regenerated if the database subsequently changes. To override the automatic probe, set configuration web_dbi_url to a different URL for the .dbi file, or to a local file:/path/to.dbi downloaded beforehand. Use the latter feature to save multiple connections from each having to fetch the .dbi separately. Lastly, set web_nodbi to true or environment SQLITE_WEB_NODBI=1 to disable dbi mode entirely. The .dbi helper is optional, but often beneficial for big databases accessed with high-latency requests. It collects bits of the main file that are key for navigating it, but typically scattered throughout (even after compaction). Prefetching them in the compact .dbi saves the reader from having to pluck them from all over the main file.","title":"Web access optimization with .dbi helper files"},{"location":"guide_db/#advice-for-big-data","text":"","title":"Advice for big data"},{"location":"guide_db/#writing-large-databases-quickly","text":"sqlite3_config(SQLITE_CONFIG_MEMSTATUS, 0) if available, to reduce overhead in SQLite3's allocation routines. Open database with unsafe_load = true to reduce transaction processing overhead (at aforementioned risk) for the connection's lifetime. Also open with the flag SQLITE_OPEN_NOMUTEX , if your application naturally serializes operations on the connection. Perform all of the following steps within one big SQLite transaction, committed at the end. Insert data rows reusing prepared, parameterized SQL statements. Process the rows in primary key order, if feasible (otherwise, see below Optimizing storage layout ). Consider preparing data in producer thread(s), with a consumer thread executing insertion statements in a tight loop. Bind text/blob parameters using SQLITE_STATIC if suitable. Create secondary indexes, including genomic range indexes, only after loading all row data. Use partial indexes when they suffice.","title":"Writing large databases quickly"},{"location":"guide_db/#optimizing-storage-layout","text":"For multiple reasons mentioned so far, large tables should have their rows initially inserted in primary key order (or whatever order will promote access locality), ensuring they'll be stored as such in the database file; and tables should be written one-at-a-time. If it's inconvenient to process the input data in this way, the following procedure can help: Create temporary table(s) with the same schema as the destination table(s), but omitting any PRIMARY KEY specifiers, UNIQUE constraints, or other indexes. Stream all the data into these temporary tables, which are fast to write and read, in whatever order is convenient. INSERT INTO permanent_table SELECT * FROM temp_table ORDER BY colA, colB, ... using the primary key (or other desired sort order) for each table. The Genomics Extension automatically enables SQLite's parallel, external merge-sorter to execute the last step efficiently. Ensure it's configured to use a suitable storage subsystem for big temporary files.","title":"Optimizing storage layout"},{"location":"guide_db/#compression-guidelines","text":"The Zstandard -based compression layer is effective at capturing the high compressibility of bioinformatics data. But, one should expect a general-purpose database to use extra space to keep everything organized, compared to a file format dedicated to one read-only schema. To set a rough expectation, the maintainers feel fairly satisfied if the database file size isn't more than double that of a bespoke compression format \u2014 especially if it includes useful indexes (which if well-designed, should be relatively incompressible). The aforementioned zstd_level, threads, and page_size options all affect the compression time-space tradeoff, while enlarging the page cache can reduce decompression overhead (workload-dependent). If you plan to delete or overwrite a significant amount of data in an existing database, issue PRAGMA secure_delete=ON beforehand to keep the compressed file as small as possible. This works by causing SQLite to overwrite unused database pages with all zeroes, which the compression layer can then reduce to a negligible size. With SQLite's row-major table storage format , the first read of a lone cell usually entails decompressing at least its whole row, and there aren't any special column encodings for deltas, run lengths, etc. The \"last mile\" of optimization may therefore involve certain schema compromises, such as storing infrequently-accessed columns in a separate table to join when needed, or using application-layer encodings with BLOB I/O .","title":"Compression guidelines"},{"location":"guide_gri/","text":"Programming Guide - Genomic Range Indexing GenomicSQLite enables creation of a Genomic Range Index (GRI) for any database table in which each row represents a genomic feature with (chromosome, beginPosition, endPosition) coordinates. The coordinates may be sourced from table columns or by computing arithmetic expressions thereof. The index tracks any updates to the underlying table as usual, with one caveat explained below. Once indexed, the table can be queried for all features overlapping a query range. A GRI query yields a rowid set, which your SQL query can select from the indexed table for further filtering or analysis. Please review the brief SQLite documentation on rowid and Autoincrement . Conventions Range positions are considered zero-based & half-open , so the length of a feature is exactly endPosition-beginPosition nucleotides. The implementation doesn't strictly require this convention, but we strongly recommend observing it to minimize confusion. There is no practical limit on chromosome length, as position values may go up to 2 60 , but queries have a runtime factor logarithmic in the maximum feature length. The extension provides routines to populate a small _gri_refseq table describing the genomic reference sequences, which other tables can reference by integer ID (\"rid\") instead of storing a column with textual sequence names like 'chr10'. This convention is not required, as the GRI can index either chromosome name or rid columns, but reasons to observe it include: Integers are more compact and faster to look up. Results sort properly with ORDER BY rid instead of considering e.g. 'chr10' < 'chr2' lexicographically. (See also the UINT collating sequence, below) A table with chromosome names can be reconstructed easily by joining with _gri_refseq . Create GRI \u21aa Create Genomic Range Index SQL: Generate a string containing a series of SQL statements which when executed create a GRI on an existing table. Executing them is left to the caller, perhaps after logging the contents. The statements should be executed within a transaction to succeed or fail atomically. Python create_gri_sql = genomicsqlite . create_genomic_range_index_sql ( dbconn , 'tableName' , 'chromosome' , 'beginPosition' , 'endPosition' ) dbconn . executescript ( create_gri_sql ) Java String griSql = GenomicSQLite . createGenomicRangeIndexSQL ( dbconn , \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); dbconn . createStatement (). executeUpdate ( griSql ); Rust let gri_sql = dbconn . create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ) ? ; dbconn . execute_batch ( & gri_sql ) ? ; SQLiteCpp std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( create_gri_sql ); C++ std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , create_gri_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * create_genomic_range_index_sql ( const char * table , const char * rid , const char * beg , const char * end , int floor ); char * create_gri_sql = create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" , - 1 ); if ( * create_gri_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , create_gri_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */ } sqlite3_free ( create_gri_sql ); The three arguments following the table name tell the indexing procedure how to read the feature coordinates from each table row. The reference sequence may be sourced either by the name of a text column containing names like 'chr10', or of an integer reference ID (rid) column, as discussed above. The begin and end positions are read from named integer columns, or by computing simple arithmetic expressions thereof. For example, if the table happens to have beginPosition and featureLength columns, the end position may be formulated 'beginPosition+featureLength' . \u2757 The table name and expressions are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. A last optional integer argument floor can be omitted or left at -1. GRI performance may be improved slightly by setting floor to a positive integer F if the following is true: the lengths of the indexed features are almost all >16 F -1 , with only very few outlier lengths \u226416 F -1 . For example, human exons are almost all >16nt; one may therefore set floor=2 as a modest optimization for such data. YMMV The indexing script will, among other steps, add a few generated columns to the original table. So if you later SELECT * FROM tableName , you'll get these extra values back (column names starting with _gri_ ). The extra columns are \"virtual\" so they don't take up space in the table itself, but they do end up populating the stored index. At present, GRI cannot be used on WITHOUT ROWID tables. Query GRI The extension supplies a special SQL function to query a GRI-indexed table, generating the set of rowids identifying features that overlap a query range (queryChrom, queryBegin, queryEnd): genomic_range_rowids(tableName, queryChrom, queryBegin, queryEnd[, ceiling, floor]) This is typically used to retrieve the result rows by selecting for tableName._rowid_ IN genomic_range_rowids(...) . For example, SELECT col1 , col2 , ... FROM exons WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 ) The queryChrom parameter might have SQL type TEXT or INTEGER, according to whether the GRI indexes name or rid. The ordered rowid set identifies the features satisfying, queryChrom = featureChrom AND NOT ( queryBegin > featureEnd OR queryEnd < featureBegin ) ( \"query is not disjoint from feature\" ) \u2757 This includes features that abut as well as those that overlap the query range, per the half-open position convention. If you don't want those, or if you want only \"contained\" features, add a WHERE clause to your query (e.g. WHERE _gri_beg >= queryBeg AND _gri_beg+_gri_len <= queryEnd ). \u2757 Results return in rowid order, which isn't necessarily genomic range order (see Advice for big data , below). Add an ORDER BY clause to your query if needed (e.g. ORDER BY _gri_rid, _gri_beg, _gri_len ). The query won't match any rows with NULL feature coordinates. If needed, the GRI can inform this query for NULL chromosome/rid: SELECT ... FROM tableName WHERE _gri_rid IS NULL . Level bounds optimization The optional, trailing ceiling & floor arguments to genomic_range_rowids() optimize GRI queries by bounding their search levels , skipping steps that'd be useless in view of the overall length distribution of the indexed features. (See Internals for full explanation.) The extension supplies a SQL helper function genomic_range_index_levels(tableName) to detect appropriate level bounds for the current version of the table. Example usage: SELECT col1 , col2 , ... FROM exons , genomic_range_index_levels ( 'exons' ) WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 , _gri_ceiling , _gri_floor ) Here _gri_ceiling and _gri_floor are columns of the single row computed by genomic_range_index_levels('exons') . genomic_range_index_levels() performs some upfront analysis of table's GRI upon its first use on any database connection. The cost of this analysis should be worthwhile if it's used to optimize many genomic_range_rowids() operations (but not just one or a few). Subsequent uses of genomic_range_index_levels() on the same connection & table reuse the first analysis, unless the database changes in the meantime, in which case the analysis must be redone. This suggests using genomic_range_index_levels() only once the database is read-only. Instead of detecting current bounds, they can be figured manually as follows. Set the integer ceiling to C , 0 < C < 16, such that all (present & future) indexed features are guaranteed to have lengths \u226416 C . For example, if you're querying features on the human genome, then you can set ceiling=7 because the lengthiest chromosome sequence is <16 7 nt. Set the integer floor F to (i) the floor value supplied at GRI creation, if any; (ii) F > 0 such that the minimum possible feature length >16 F -1 , if any; or (iii) zero. The safe, default bounds are C=15, F=0. GRI queries with inappropriate bounds are liable to produce incomplete results. Joining tables on range overlap Suppose we have two tables with genomic features to join on range overlap. Only the \"right-hand\" table must have a GRI; preferably the smaller of the two. For example, annotating a table of variants with the surrounding exon(s), if any: SELECT variants . * , exons . _rowid_ FROM variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos ) We fill out the GRI query range using the three coordinate columns of the variants table. We may be able to speed this up by supplying level bounds, as discussed above: SELECT variants . * , exons . _rowid_ FROM genomic_range_index_levels ( 'exons' ), variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos , _gri_ceiling , _gri_floor ) See also \"Advice for big data\" below on optimizing storage layout for GRI queries. Reference genome metadata The following routines support the aforementioned, recommended convention for storing a _gri_refseq table with information about the genomic reference sequences, which other tables can cross-reference by integer ID (rid) instead of storing textual chromosome names. The columns of _gri_refseq include: _gri_rid INTEGER PRIMARY KEY gri_refseq_name TEXT NOT NULL gri_refseq_length INTEGER NOT NULL gri_assembly TEXT genome assembly name (optional) gri_refget_id TEXT refget sequence ID (optional) gri_refseq_meta_json TEXT DEFAULT '{}' JSON object with arbitrary metadata \u21aa Put Reference Assembly SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq and populates it with information about a reference assembly whose details are bundled into the extension. Python refseq_sql = genomicsqlite . put_reference_assembly_sql ( dbconn , 'GRCh38_no_alt_analysis_set' ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceAssemblySQL ( dbconn , \"GRCh38_no_alt_analysis_set\" ); dbconn . createStatement (). executeUpdate ( refSql ); Rust let ref_sql = dbconn . put_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" ) ? ; dbconn . execute_batch ( & ref_sql ) ? ; SQLiteCpp std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" )); C++ std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_assembly_sql ( const char * assembly , const char * attached_schema ); char * refseq_sql = put_genomic_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" , nullptr ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); Available assemblies: GRCh38_no_alt_analysis_set \u21aa Put Reference Sequence SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq (if it doesn't exist) and adds one reference sequence with supplied attributes. Python refseq_sql = genomicsqlite . put_reference_sequence_sql ( dbconn , 'chr17' , 83257441 # optional: assembly, refget_id, meta (dict), rid ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceSequenceSQL ( dbconn , \"chr17\" , 83257441L // optional overloads: // String assembly, String refget_id, String meta_json, long rid ); dbconn . createStatement (). executeUpdate ( refSql ); Rust let chr17 = genomicsqlite :: RefSeq { rid : - 1 , // -1 = automatic name : \"chr17\" , length : 83257441 , assembly : None , // Option<String> refget_id : None , // Option<String> meta_json : json :: object :: Object :: new (), // meta_json }; let ref_sql = dbconn . put_reference_sequence_sql ( & chr17 ) ? ; dbconn . execute_batch ( & ref_sql ) ? ; SQLiteCpp std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceSequenceSQL ( \"chr17\" , 83257441 )); C++ std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"chr17\" , 83257441 ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_sequence_sql ( const char * name , sqlite3_int64 length , const char * assembly , const char * refget_id , const char * meta_json , sqlite3_int64 rid , const char * attached_schema ); char * refseq_sql = put_genomic_reference_sequence_sql ( \"chr17\" , 83257441 , 0 , 0 , 0 , - 1 , 0 ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); If the rid argument is omitted or -1 then it will be assigned automatically upon insertion. \u21aa Get Reference Sequences by Rid: create an in-memory lookup table of the previously-stored reference information, keyed by rid integer. Assumes the stored information is read-only by this point. This table is for the application code's convenience to read tables that use the rid convention. Such uses can be also be served by SQL join on the _gri_refseq table (see Cookbook). Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_rid = genomicsqlite . get_reference_sequences_by_rid ( dbconn ) # refseq_by_rid: Dict[int, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < Long , ReferenceSequence > refseqByRid = GenomicSQLite . getReferenceSequencesByRid ( dbconn ); Rust /* struct RefSeq { rid: i64, name: String, length: i64, assembly: Option<String>, refget_id: Option<String>, meta_json: json::object::Object, } */ let refseqs : HashMap < i64 , genomicsqlite :: RefSeq > = dbconn . get_reference_sequences_by_rid () ? ; SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ The optional assembly argument restricts the retrieved sequences to those with matching gri_assembly value. However, mixing different assemblies in _gri_refseq is not recommended. \u21aa Get Reference Sequences by Name: create an in-memory lookup table of the previously-stored reference information, keyed by sequence name. Assumes the stored information is read-only by this point. This table is for the application code's convenience to translate name to rid whilst formulating queries or inserting features from a text source. Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_name = genomicsqlite . get_reference_sequences_by_name ( dbconn ) # refseq_by_name: Dict[str, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < String , ReferenceSequence > refseqByName = GenomicSQLite . getReferenceSequencesByName ( dbconn ); Rust /* struct RefSeq { rid: i64, name: String, length: i64, assembly: Option<String>, refget_id: Option<String>, meta_json: json::object::Object, } */ let refseqs : HashMap < String , genomicsqlite :: RefSeq > = dbconn . get_reference_sequences_by_name () ? ; SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ Cookbook rid to chromosome name Table identifies each feature's chromosome by rid, and we want to see them with text chromosome names. SELECT gri_refseq_name , feature_table . * FROM feature_table NATURAL JOIN _gri_refseq The join key here is _gri_rid , which is one of the generated columns added by GRI creation. Alternatively, the application code can read rid from the row and translate it using the lookup table generated by the Get Reference Sequences by Rid routine. Query rid using chromosome name We're making a GRI query on a table that stores rid integers, but our query range has a chromosome name. SELECT feature_table . * FROM ( SELECT _gri_rid AS rid FROM _gri_refseq WHERE gri_refseq_name = 'chr12' ) AS query , feature_table WHERE feature_table . _rowid_ IN genomic_range_rowids ( 'feature_table' , query . rid , 111803912 , 111804012 ) We use a subquery to look up the rid corresponding to the known chromosome name. Alternatively, the application code can first convert the query name to rid using the lookup table generated by the Get Reference Sequences by Name routine. Circular chromosome query On circular chromosomes, range queries should include features that wrap around the origin to end inside the desired range. If we've stored them naively with featureEnd = featureBegin + featureLength, then we can build a unified query with reference to the stored chromosome lengths: SELECT col1 , col2 , ... FROM ( SELECT gri_refseq_length FROM _gri_refseq WHERE _gri_rid = queryRid ), featureTable WHERE featureTable . _rowid_ IN ( genomic_range_rowids ( 'featureTable' , queryRid , queryBegin , queryEnd ) UNION genomic_range_rowids ( 'featureTable' , queryRid , gri_refseq_length + queryBegin , gri_refseq_length + queryEnd )) We query a second range beyond the chromosome length, which will match features that wrap around into the query. UNION deduplicates the result rowids. As a convention, set \"circular\": true in the _gri_refseq.gri_refseq_meta_json for circular chromosomes. Advice for big data The database file stores tables in rowid order (effectively). It's therefore preferable for a mainly-GRI-queried table to be written in genomic range order, so that the features' (chromosome, beginPosition) monotonically increase with rowid, and range queries enjoy storage/cache locality. See Optimizing storage layout in the compression guide for advice if it isn't straightforward to initally write the rows ordered by (chromosome, beginPosition). Though not required in theory, this may be needed in practice for GRI queries that will match a material fraction of a big table's rows. A series of many GRI queries (including in service of a join) should also proceed in genomic range order. If this isn't possible, then ideally the database page cache should be enlarged to fit the entire indexed table in memory. If you expect a GRI query to yield a very large, contiguous rowid result set (e.g. all features on a chromosome, in a table known to be range-sorted), then the following specialized query plan may be advantageous: Ask GRI for first relevant rowid, SELECT MIN(_rowid_) AS firstRowid FROM genomic_range_rowids(...) Open a cursor on SELECT ... FROM tableName WHERE _rowid_ >= firstRowid Loop through rows for as long as they're relevant. But this plan strongly depends on the contiguity assumption.","title":"Genomic Range Indexing"},{"location":"guide_gri/#programming-guide-genomic-range-indexing","text":"GenomicSQLite enables creation of a Genomic Range Index (GRI) for any database table in which each row represents a genomic feature with (chromosome, beginPosition, endPosition) coordinates. The coordinates may be sourced from table columns or by computing arithmetic expressions thereof. The index tracks any updates to the underlying table as usual, with one caveat explained below. Once indexed, the table can be queried for all features overlapping a query range. A GRI query yields a rowid set, which your SQL query can select from the indexed table for further filtering or analysis. Please review the brief SQLite documentation on rowid and Autoincrement .","title":"Programming Guide - Genomic Range Indexing"},{"location":"guide_gri/#conventions","text":"Range positions are considered zero-based & half-open , so the length of a feature is exactly endPosition-beginPosition nucleotides. The implementation doesn't strictly require this convention, but we strongly recommend observing it to minimize confusion. There is no practical limit on chromosome length, as position values may go up to 2 60 , but queries have a runtime factor logarithmic in the maximum feature length. The extension provides routines to populate a small _gri_refseq table describing the genomic reference sequences, which other tables can reference by integer ID (\"rid\") instead of storing a column with textual sequence names like 'chr10'. This convention is not required, as the GRI can index either chromosome name or rid columns, but reasons to observe it include: Integers are more compact and faster to look up. Results sort properly with ORDER BY rid instead of considering e.g. 'chr10' < 'chr2' lexicographically. (See also the UINT collating sequence, below) A table with chromosome names can be reconstructed easily by joining with _gri_refseq .","title":"Conventions"},{"location":"guide_gri/#create-gri","text":"\u21aa Create Genomic Range Index SQL: Generate a string containing a series of SQL statements which when executed create a GRI on an existing table. Executing them is left to the caller, perhaps after logging the contents. The statements should be executed within a transaction to succeed or fail atomically. Python create_gri_sql = genomicsqlite . create_genomic_range_index_sql ( dbconn , 'tableName' , 'chromosome' , 'beginPosition' , 'endPosition' ) dbconn . executescript ( create_gri_sql ) Java String griSql = GenomicSQLite . createGenomicRangeIndexSQL ( dbconn , \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); dbconn . createStatement (). executeUpdate ( griSql ); Rust let gri_sql = dbconn . create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ) ? ; dbconn . execute_batch ( & gri_sql ) ? ; SQLiteCpp std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( create_gri_sql ); C++ std :: string CreateGenomicRangeIndexSQL ( const std :: string & table , const std :: string & rid , const std :: string & beg , const std :: string & end , int floor = - 1 ); std :: string create_gri_sql = CreateGenomicRangeIndexSQL ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , create_gri_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * create_genomic_range_index_sql ( const char * table , const char * rid , const char * beg , const char * end , int floor ); char * create_gri_sql = create_genomic_range_index_sql ( \"tableName\" , \"chromosome\" , \"beginPosition\" , \"endPosition\" , - 1 ); if ( * create_gri_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , create_gri_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* General note: all GenomicSQLite C routines returning a char* string use * the following convention: * If the operation suceeds then it's a nonempty, null-terminated string. * Otherwise it points to a null byte followed immediately by a nonempty, * null-terminated error message. * IN EITHER CASE, the caller should free the string with sqlite3_free(). * Null is returned only if malloc failed. */ } sqlite3_free ( create_gri_sql ); The three arguments following the table name tell the indexing procedure how to read the feature coordinates from each table row. The reference sequence may be sourced either by the name of a text column containing names like 'chr10', or of an integer reference ID (rid) column, as discussed above. The begin and end positions are read from named integer columns, or by computing simple arithmetic expressions thereof. For example, if the table happens to have beginPosition and featureLength columns, the end position may be formulated 'beginPosition+featureLength' . \u2757 The table name and expressions are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. A last optional integer argument floor can be omitted or left at -1. GRI performance may be improved slightly by setting floor to a positive integer F if the following is true: the lengths of the indexed features are almost all >16 F -1 , with only very few outlier lengths \u226416 F -1 . For example, human exons are almost all >16nt; one may therefore set floor=2 as a modest optimization for such data. YMMV The indexing script will, among other steps, add a few generated columns to the original table. So if you later SELECT * FROM tableName , you'll get these extra values back (column names starting with _gri_ ). The extra columns are \"virtual\" so they don't take up space in the table itself, but they do end up populating the stored index. At present, GRI cannot be used on WITHOUT ROWID tables.","title":"Create GRI"},{"location":"guide_gri/#query-gri","text":"The extension supplies a special SQL function to query a GRI-indexed table, generating the set of rowids identifying features that overlap a query range (queryChrom, queryBegin, queryEnd): genomic_range_rowids(tableName, queryChrom, queryBegin, queryEnd[, ceiling, floor]) This is typically used to retrieve the result rows by selecting for tableName._rowid_ IN genomic_range_rowids(...) . For example, SELECT col1 , col2 , ... FROM exons WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 ) The queryChrom parameter might have SQL type TEXT or INTEGER, according to whether the GRI indexes name or rid. The ordered rowid set identifies the features satisfying, queryChrom = featureChrom AND NOT ( queryBegin > featureEnd OR queryEnd < featureBegin ) ( \"query is not disjoint from feature\" ) \u2757 This includes features that abut as well as those that overlap the query range, per the half-open position convention. If you don't want those, or if you want only \"contained\" features, add a WHERE clause to your query (e.g. WHERE _gri_beg >= queryBeg AND _gri_beg+_gri_len <= queryEnd ). \u2757 Results return in rowid order, which isn't necessarily genomic range order (see Advice for big data , below). Add an ORDER BY clause to your query if needed (e.g. ORDER BY _gri_rid, _gri_beg, _gri_len ). The query won't match any rows with NULL feature coordinates. If needed, the GRI can inform this query for NULL chromosome/rid: SELECT ... FROM tableName WHERE _gri_rid IS NULL .","title":"Query GRI"},{"location":"guide_gri/#level-bounds-optimization","text":"The optional, trailing ceiling & floor arguments to genomic_range_rowids() optimize GRI queries by bounding their search levels , skipping steps that'd be useless in view of the overall length distribution of the indexed features. (See Internals for full explanation.) The extension supplies a SQL helper function genomic_range_index_levels(tableName) to detect appropriate level bounds for the current version of the table. Example usage: SELECT col1 , col2 , ... FROM exons , genomic_range_index_levels ( 'exons' ) WHERE exons . _rowid_ IN genomic_range_rowids ( 'exons' , 'chr12' , 111803912 , 111804012 , _gri_ceiling , _gri_floor ) Here _gri_ceiling and _gri_floor are columns of the single row computed by genomic_range_index_levels('exons') . genomic_range_index_levels() performs some upfront analysis of table's GRI upon its first use on any database connection. The cost of this analysis should be worthwhile if it's used to optimize many genomic_range_rowids() operations (but not just one or a few). Subsequent uses of genomic_range_index_levels() on the same connection & table reuse the first analysis, unless the database changes in the meantime, in which case the analysis must be redone. This suggests using genomic_range_index_levels() only once the database is read-only. Instead of detecting current bounds, they can be figured manually as follows. Set the integer ceiling to C , 0 < C < 16, such that all (present & future) indexed features are guaranteed to have lengths \u226416 C . For example, if you're querying features on the human genome, then you can set ceiling=7 because the lengthiest chromosome sequence is <16 7 nt. Set the integer floor F to (i) the floor value supplied at GRI creation, if any; (ii) F > 0 such that the minimum possible feature length >16 F -1 , if any; or (iii) zero. The safe, default bounds are C=15, F=0. GRI queries with inappropriate bounds are liable to produce incomplete results.","title":"Level bounds optimization"},{"location":"guide_gri/#joining-tables-on-range-overlap","text":"Suppose we have two tables with genomic features to join on range overlap. Only the \"right-hand\" table must have a GRI; preferably the smaller of the two. For example, annotating a table of variants with the surrounding exon(s), if any: SELECT variants . * , exons . _rowid_ FROM variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos ) We fill out the GRI query range using the three coordinate columns of the variants table. We may be able to speed this up by supplying level bounds, as discussed above: SELECT variants . * , exons . _rowid_ FROM genomic_range_index_levels ( 'exons' ), variants LEFT JOIN exons ON exons . _rowid_ IN genomic_range_rowids ( 'exons' , variants . chrom , variants . beginPos , variants . endPos , _gri_ceiling , _gri_floor ) See also \"Advice for big data\" below on optimizing storage layout for GRI queries.","title":"Joining tables on range overlap"},{"location":"guide_gri/#reference-genome-metadata","text":"The following routines support the aforementioned, recommended convention for storing a _gri_refseq table with information about the genomic reference sequences, which other tables can cross-reference by integer ID (rid) instead of storing textual chromosome names. The columns of _gri_refseq include: _gri_rid INTEGER PRIMARY KEY gri_refseq_name TEXT NOT NULL gri_refseq_length INTEGER NOT NULL gri_assembly TEXT genome assembly name (optional) gri_refget_id TEXT refget sequence ID (optional) gri_refseq_meta_json TEXT DEFAULT '{}' JSON object with arbitrary metadata \u21aa Put Reference Assembly SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq and populates it with information about a reference assembly whose details are bundled into the extension. Python refseq_sql = genomicsqlite . put_reference_assembly_sql ( dbconn , 'GRCh38_no_alt_analysis_set' ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceAssemblySQL ( dbconn , \"GRCh38_no_alt_analysis_set\" ); dbconn . createStatement (). executeUpdate ( refSql ); Rust let ref_sql = dbconn . put_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" ) ? ; dbconn . execute_batch ( & ref_sql ) ? ; SQLiteCpp std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" )); C++ std :: string PutGenomicReferenceAssemblySQL ( const std :: string & assembly , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"GRCh38_no_alt_analysis_set\" ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_assembly_sql ( const char * assembly , const char * attached_schema ); char * refseq_sql = put_genomic_reference_assembly_sql ( \"GRCh38_no_alt_analysis_set\" , nullptr ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); Available assemblies: GRCh38_no_alt_analysis_set \u21aa Put Reference Sequence SQL: Generate a string containing a series of SQL statements which when executed creates _gri_refseq (if it doesn't exist) and adds one reference sequence with supplied attributes. Python refseq_sql = genomicsqlite . put_reference_sequence_sql ( dbconn , 'chr17' , 83257441 # optional: assembly, refget_id, meta (dict), rid ) dbconn . executescript ( refseq_sql ) Java String refSql = GenomicSQLite . putReferenceSequenceSQL ( dbconn , \"chr17\" , 83257441L // optional overloads: // String assembly, String refget_id, String meta_json, long rid ); dbconn . createStatement (). executeUpdate ( refSql ); Rust let chr17 = genomicsqlite :: RefSeq { rid : - 1 , // -1 = automatic name : \"chr17\" , length : 83257441 , assembly : None , // Option<String> refget_id : None , // Option<String> meta_json : json :: object :: Object :: new (), // meta_json }; let ref_sql = dbconn . put_reference_sequence_sql ( & chr17 ) ? ; dbconn . execute_batch ( & ref_sql ) ? ; SQLiteCpp std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn in a transaction dbconn -> exec ( PutGenomicReferenceSequenceSQL ( \"chr17\" , 83257441 )); C++ std :: string PutGenomicReferenceSequenceSQL ( const std :: string & name , sqlite3_int64 length , const std :: string & assembly = \"\" , const std :: string & refget_id = \"\" , const std :: string & meta_json = \"{}\" , sqlite3_int64 rid = - 1 , const std :: string & attached_schema = \"\" ); std :: string refseq_sql = PutGenomicReferenceAssemblySQL ( \"chr17\" , 83257441 ); // sqlite3* dbconn in a transaction char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , refseq_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg C char * put_genomic_reference_sequence_sql ( const char * name , sqlite3_int64 length , const char * assembly , const char * refget_id , const char * meta_json , sqlite3_int64 rid , const char * attached_schema ); char * refseq_sql = put_genomic_reference_sequence_sql ( \"chr17\" , 83257441 , 0 , 0 , 0 , - 1 , 0 ); if ( * refseq_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn in a transaction */ int rc = sqlite3_exec ( dbconn , refseq_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( refseq_sql ); If the rid argument is omitted or -1 then it will be assigned automatically upon insertion. \u21aa Get Reference Sequences by Rid: create an in-memory lookup table of the previously-stored reference information, keyed by rid integer. Assumes the stored information is read-only by this point. This table is for the application code's convenience to read tables that use the rid convention. Such uses can be also be served by SQL join on the _gri_refseq table (see Cookbook). Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_rid = genomicsqlite . get_reference_sequences_by_rid ( dbconn ) # refseq_by_rid: Dict[int, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < Long , ReferenceSequence > refseqByRid = GenomicSQLite . getReferenceSequencesByRid ( dbconn ); Rust /* struct RefSeq { rid: i64, name: String, length: i64, assembly: Option<String>, refget_id: Option<String>, meta_json: json::object::Object, } */ let refseqs : HashMap < i64 , genomicsqlite :: RefSeq > = dbconn . get_reference_sequences_by_rid () ? ; SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < long long , gri_refseq_t > GetGenomicReferenceSequencesByRid ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_rid = GetGenomicReferenceSequencesByRid ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */ The optional assembly argument restricts the retrieved sequences to those with matching gri_assembly value. However, mixing different assemblies in _gri_refseq is not recommended. \u21aa Get Reference Sequences by Name: create an in-memory lookup table of the previously-stored reference information, keyed by sequence name. Assumes the stored information is read-only by this point. This table is for the application code's convenience to translate name to rid whilst formulating queries or inserting features from a text source. Python class ReferenceSequence ( NamedTuple ): rid : int name : str length : int assembly : Optional [ str ] refget_id : Optional [ str ] meta : Dict [ str , Any ] refseq_by_name = genomicsqlite . get_reference_sequences_by_name ( dbconn ) # refseq_by_name: Dict[str, ReferenceSequence] Java import java.util.HashMap ; import net.mlin.genomicsqlite.ReferenceSequence ; /* public class ReferenceSequence { public final long rid, length; public final String name, assembly, refgetId, metaJson; } */ HashMap < String , ReferenceSequence > refseqByName = GenomicSQLite . getReferenceSequencesByName ( dbconn ); Rust /* struct RefSeq { rid: i64, name: String, length: i64, assembly: Option<String>, refget_id: Option<String>, meta_json: json::object::Object, } */ let refseqs : HashMap < String , genomicsqlite :: RefSeq > = dbconn . get_reference_sequences_by_name () ? ; SQLiteCpp struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // SQLite::Database* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn -> getHandle ()); C++ struct gri_refseq_t { long long rid , length ; std :: string name , assembly , refget_id , meta_json ; }; std :: map < std :: string , gri_refseq_t > GetGenomicReferenceSequencesByName ( sqlite3 * dbconn , const std :: string & assembly = \"\" , const std :: string & attached_schema = \"\" ); // sqlite3* dbconn auto refseq_by_name = GetGenomicReferenceSequencesByName ( dbconn ); C /* Omitted for want of idiomatic map type; pull requests welcome! */","title":"Reference genome metadata"},{"location":"guide_gri/#cookbook","text":"","title":"Cookbook"},{"location":"guide_gri/#rid-to-chromosome-name","text":"Table identifies each feature's chromosome by rid, and we want to see them with text chromosome names. SELECT gri_refseq_name , feature_table . * FROM feature_table NATURAL JOIN _gri_refseq The join key here is _gri_rid , which is one of the generated columns added by GRI creation. Alternatively, the application code can read rid from the row and translate it using the lookup table generated by the Get Reference Sequences by Rid routine.","title":"rid to chromosome name"},{"location":"guide_gri/#query-rid-using-chromosome-name","text":"We're making a GRI query on a table that stores rid integers, but our query range has a chromosome name. SELECT feature_table . * FROM ( SELECT _gri_rid AS rid FROM _gri_refseq WHERE gri_refseq_name = 'chr12' ) AS query , feature_table WHERE feature_table . _rowid_ IN genomic_range_rowids ( 'feature_table' , query . rid , 111803912 , 111804012 ) We use a subquery to look up the rid corresponding to the known chromosome name. Alternatively, the application code can first convert the query name to rid using the lookup table generated by the Get Reference Sequences by Name routine.","title":"Query rid using chromosome name"},{"location":"guide_gri/#circular-chromosome-query","text":"On circular chromosomes, range queries should include features that wrap around the origin to end inside the desired range. If we've stored them naively with featureEnd = featureBegin + featureLength, then we can build a unified query with reference to the stored chromosome lengths: SELECT col1 , col2 , ... FROM ( SELECT gri_refseq_length FROM _gri_refseq WHERE _gri_rid = queryRid ), featureTable WHERE featureTable . _rowid_ IN ( genomic_range_rowids ( 'featureTable' , queryRid , queryBegin , queryEnd ) UNION genomic_range_rowids ( 'featureTable' , queryRid , gri_refseq_length + queryBegin , gri_refseq_length + queryEnd )) We query a second range beyond the chromosome length, which will match features that wrap around into the query. UNION deduplicates the result rowids. As a convention, set \"circular\": true in the _gri_refseq.gri_refseq_meta_json for circular chromosomes.","title":"Circular chromosome query"},{"location":"guide_gri/#advice-for-big-data","text":"The database file stores tables in rowid order (effectively). It's therefore preferable for a mainly-GRI-queried table to be written in genomic range order, so that the features' (chromosome, beginPosition) monotonically increase with rowid, and range queries enjoy storage/cache locality. See Optimizing storage layout in the compression guide for advice if it isn't straightforward to initally write the rows ordered by (chromosome, beginPosition). Though not required in theory, this may be needed in practice for GRI queries that will match a material fraction of a big table's rows. A series of many GRI queries (including in service of a join) should also proceed in genomic range order. If this isn't possible, then ideally the database page cache should be enlarged to fit the entire indexed table in memory. If you expect a GRI query to yield a very large, contiguous rowid result set (e.g. all features on a chromosome, in a table known to be range-sorted), then the following specialized query plan may be advantageous: Ask GRI for first relevant rowid, SELECT MIN(_rowid_) AS firstRowid FROM genomic_range_rowids(...) Open a cursor on SELECT ... FROM tableName WHERE _rowid_ >= firstRowid Loop through rows for as long as they're relevant. But this plan strongly depends on the contiguity assumption.","title":"Advice for big data"},{"location":"guide_helpers/","text":"Programming Guide - Useful Routines DNA reverse complement Reverse-complements a DNA text value (containing only characters from the set AGCTagct ), preserving original case. SQL SELECT dna_revcomp ( 'AGCTagct' ) -- 'agctAGCT' Given NULL, returns NULL. Any other input is an error. Parse genomic range text These SQL functions process a text value like 'chr1:2,345-6,789' into its three parts (sequence/chromosome name, begin position, and end position). SQL SELECT parse_genomic_range_sequence ( 'chr1:2,345-6,789' ) -- 'chr1' SELECT parse_genomic_range_begin ( 'chr1:2,345-6,789' ) -- 2344 (!) SELECT parse_genomic_range_end ( 'chr1:2,345-6,789' ) -- 6789 \u2757 Since such text ranges are conventionally one-based and closed, parse_genomic_range_begin() effectively converts them to zero-based and half-open by returning one less than the text begin position . Given NULL, each function returns NULL. An error is raised if the text value can't be parsed, or for any other input type. Two-bit encoding for nucleotide sequences The extension supplies SQL functions to pack a DNA/RNA sequence TEXT value into a smaller BLOB value, using two bits per nucleotide. (Review SQLite Datatypes on the important differences between TEXT and BLOB values & columns.) Storing a large database of sequences using such BLOBs instead of TEXT can improve application I/O efficiency, with up to 4X more nucleotides cached in the same memory space. It is not, however, expected to greatly shrink the database file on disk, owing to the automatic storage compression. The encoding is case-insensitive and considers T and U equivalent. Encoding: SQL SELECT nucleotides_twobit ( 'TCAG' ) Given a TEXT value consisting of characters from the set ACGTUacgtu , compute a two-bit-encoded BLOB value that can later be decoded using twobit_dna() or twobit_rna() . Given any other ASCII TEXT value (including empty), pass it through unchanged as TEXT. Given NULL, return NULL. Any other input is an error. Typically used to populate a BLOB column C with e.g. INSERT INTO some_table (..., C ) VALUES (..., nucleotides_twobit ( ? )) This works even if some of the sequences contain N s or other characters, in which case those sequences are stored as the original TEXT values. Make sure the column has schema type BLOB to avoid spurious coercions, and by convention, the column should be named *_twobit. Decoding: SQL SELECT twobit_dna ( nucleotides_twobit ( 'TCAG' )) SELECT twobit_rna ( nucleotides_twobit ( 'UCAG' )) SELECT twobit_dna ( nucleotides_twobit ( 'TCAG' ), Y , Z ) SELECT twobit_rna ( nucleotides_twobit ( 'UCAG' ), Y , Z ) Given a two-bit-encoded BLOB value, decode the nucleotide sequence as uppercased TEXT, with T 's for twobit_dna() and U 's for twobit_rna() . Given a TEXT value, pass it through unchanged. Given NULL, return NULL. Any other first input is an error. The optional Y and Z arguments can be used to compute substr(twobit_dna(X),Y,Z) more efficiently, without decoding the whole sequence. Unfortunately however, SQLite internals make this operation still liable to use time & memory proportional to the full length of X, not Z. If frequent random access into long sequences is needed, then consider splitting them across multiple rows. Take care to only use BLOBs originally produced by nucleotides_twobit() , as other BLOBs may decode to spurious nucleotide sequences. If you SELECT twobit_dna(C) FROM some_table on a column with mixed BLOB and TEXT values as suggested above, note that the results actually stored as TEXT preserve their case and T/U letters, unlike decoded BLOBs. Length: SQL SELECT twobit_length ( dna_twobit ( 'TCAG' )) Given a two-bit-encoded BLOB value, return the length of the decoded sequence (without actually decoding it). This is not equal to 4*length(BLOB) due to padding. Given a TEXT value, return its byte length. Given NULL, return NULL. Any other input is an error. JSON1 and UINT extensions The Genomics Extension bundles the SQLite developers' JSON1 extension and enables it automatically. By convention, JSON object columns should be named *_json and JSON array columns should be named *_jsarray. The JSON1 functions can be used with generated columns to effectively allow indexing of JSON-embedded fields. The UINT collating sequence is also bundled. This can be useful to make e.g. ORDER BY chromosome COLLATE UINT put 'chr2' before 'chr10'. Attach GenomicSQLite database \u21aa GenomicSQLite Attach: Generate a string containing a series of SQL statements to execute on an existing database connection in order to ATTACH a GenomicSQLite database under a given schema name. The main connection may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'any.db' , uri = True ) attach_sql = genomicsqlite . attach_sql ( dbconn , 'compressed.db' , 'db2' ) # attach_sql() also takes configuration keyword arguments like # genomicsqlite.connect() dbconn . executescript ( attach_sql ) # compressed.db now attached as db2 Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:any.db\" ); String attachSql = GenomicSQLite . attachSQL ( dbconn , \"compressed.db\" , \"db2\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( attachSql ); // compressed.db now attached as db2 Rust let genomicsqlite_options = json :: object :: Object :: new (); // If needed, trigger initial load of Genomics Extension prior to // opening other connections let nop = genomicsqlite :: open ( \":memory:\" , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; // dbconn: rusqlite::Connection with SQLITE_OPEN_URI let attach_sql = dbconn . genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , & genomicsqlite_options ) ? ; dbconn . execute_batch ( & attach_sql ) ? ; // compressed.db is now attached as db2 SQLiteCpp std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); SQLite :: Database dbconn ( \"any.db\" , SQLITE_OPEN_URI ); dbconn . exec ( attach_sql ); // compressed.db now attached as db2 C++ std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , attach_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // compressed.db now attached as db2 C char * genomicsqlite_attach_sql ( const char * dbfile , const char * schema_name , const char * config_json ); char * attach_sql = genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , \"{}\" ); if ( * attach_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , attach_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( attach_sql ); /* compressed.db now attached as db2 */ \u2757 The file and schema names are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input. Compress existing SQLite3 database \u21aa GenomicSQLite Vacuum Into: Generate a string containing a series of SQL statements to execute on an existing database in order to copy it into a new compressed & defragmented file. The source database may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'existing.db' , uri = True ) vacuum_sql = genomicsqlite . vacuum_into_sql ( dbconn , 'compressed.db' ) # vacuum_into_sql() also takes configuration keyword arguments like # genomicsqlite.connect() to control compression level & page sizes dbconn . executescript ( vacuum_sql ) dbconn2 = genomicsqlite . connect ( 'compressed.db' ) Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:existing.db\" ); String vacuumSql = GenomicSQLite . vacuumIntoSQL ( dbconn , \"compressed.db\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( vaccumSql ); Connection dbconn2 = DriverManager . getConnection ( \"jdbc:genomicsqlite:compressed.db\" ); Rust let genomicsqlite_options = json :: object :: Object :: new (); // If needed, trigger initial load of Genomics Extension prior to // opening other connections let nop = genomicsqlite :: open ( \":memory:\" , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; // dbconn: rusqlite::Connection with SQLITE_OPEN_URI let vacuum_sql = dbconn . genomicsqlite_vacuum_into_sql ( \"compressed.db\" , & genomicsqlite_options ) ? ; dbconn . execute_batch ( & vacuum_sql ) ? ; let dbconn2 = genomicsqlite :: open ( \"compressed.db\" , OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; SQLiteCpp std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); SQLite :: Database dbconn ( \"existing.db\" , SQLITE_OPEN_READONLY | SQLITE_OPEN_URI ); dbconn . exec ( vacuum_sql ); auto dbconn2 = GenomicSQLiteOpen ( \"compressed.db\" ); C++ std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , vacuum_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // rc = GenomicSQLiteOpen(\"compressed.db\", ...); C char * genomicsqlite_vacuum_into_sql ( const char * dest_filename , const char * config_json ); char * vacuum_sql = genomicsqlite_vacuum_into_sql ( \"compressed.db\" , \"{}\" ); if ( * vacuum_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , vacuum_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( vacuum_sql ); /* genomicsqlite_open(\"compressed.db\", ...); */ Genomics Extension version SQL SELECT genomicsqlite_version () Python genomicsqlite . __version__ Java String genomicsqliteVersion = GenomicSQLite . version ( dbconn ); Rust let v : String = dbconn . genomicsqlite_version (); C++ std :: string GenomicSQLiteVersion (); C char * genomicsqlite_version (); /* result to be sqlite3_free() */","title":"Useful Routines"},{"location":"guide_helpers/#programming-guide-useful-routines","text":"","title":"Programming Guide - Useful Routines"},{"location":"guide_helpers/#dna-reverse-complement","text":"Reverse-complements a DNA text value (containing only characters from the set AGCTagct ), preserving original case. SQL SELECT dna_revcomp ( 'AGCTagct' ) -- 'agctAGCT' Given NULL, returns NULL. Any other input is an error.","title":"DNA reverse complement"},{"location":"guide_helpers/#parse-genomic-range-text","text":"These SQL functions process a text value like 'chr1:2,345-6,789' into its three parts (sequence/chromosome name, begin position, and end position). SQL SELECT parse_genomic_range_sequence ( 'chr1:2,345-6,789' ) -- 'chr1' SELECT parse_genomic_range_begin ( 'chr1:2,345-6,789' ) -- 2344 (!) SELECT parse_genomic_range_end ( 'chr1:2,345-6,789' ) -- 6789 \u2757 Since such text ranges are conventionally one-based and closed, parse_genomic_range_begin() effectively converts them to zero-based and half-open by returning one less than the text begin position . Given NULL, each function returns NULL. An error is raised if the text value can't be parsed, or for any other input type.","title":"Parse genomic range text"},{"location":"guide_helpers/#two-bit-encoding-for-nucleotide-sequences","text":"The extension supplies SQL functions to pack a DNA/RNA sequence TEXT value into a smaller BLOB value, using two bits per nucleotide. (Review SQLite Datatypes on the important differences between TEXT and BLOB values & columns.) Storing a large database of sequences using such BLOBs instead of TEXT can improve application I/O efficiency, with up to 4X more nucleotides cached in the same memory space. It is not, however, expected to greatly shrink the database file on disk, owing to the automatic storage compression. The encoding is case-insensitive and considers T and U equivalent. Encoding: SQL SELECT nucleotides_twobit ( 'TCAG' ) Given a TEXT value consisting of characters from the set ACGTUacgtu , compute a two-bit-encoded BLOB value that can later be decoded using twobit_dna() or twobit_rna() . Given any other ASCII TEXT value (including empty), pass it through unchanged as TEXT. Given NULL, return NULL. Any other input is an error. Typically used to populate a BLOB column C with e.g. INSERT INTO some_table (..., C ) VALUES (..., nucleotides_twobit ( ? )) This works even if some of the sequences contain N s or other characters, in which case those sequences are stored as the original TEXT values. Make sure the column has schema type BLOB to avoid spurious coercions, and by convention, the column should be named *_twobit. Decoding: SQL SELECT twobit_dna ( nucleotides_twobit ( 'TCAG' )) SELECT twobit_rna ( nucleotides_twobit ( 'UCAG' )) SELECT twobit_dna ( nucleotides_twobit ( 'TCAG' ), Y , Z ) SELECT twobit_rna ( nucleotides_twobit ( 'UCAG' ), Y , Z ) Given a two-bit-encoded BLOB value, decode the nucleotide sequence as uppercased TEXT, with T 's for twobit_dna() and U 's for twobit_rna() . Given a TEXT value, pass it through unchanged. Given NULL, return NULL. Any other first input is an error. The optional Y and Z arguments can be used to compute substr(twobit_dna(X),Y,Z) more efficiently, without decoding the whole sequence. Unfortunately however, SQLite internals make this operation still liable to use time & memory proportional to the full length of X, not Z. If frequent random access into long sequences is needed, then consider splitting them across multiple rows. Take care to only use BLOBs originally produced by nucleotides_twobit() , as other BLOBs may decode to spurious nucleotide sequences. If you SELECT twobit_dna(C) FROM some_table on a column with mixed BLOB and TEXT values as suggested above, note that the results actually stored as TEXT preserve their case and T/U letters, unlike decoded BLOBs. Length: SQL SELECT twobit_length ( dna_twobit ( 'TCAG' )) Given a two-bit-encoded BLOB value, return the length of the decoded sequence (without actually decoding it). This is not equal to 4*length(BLOB) due to padding. Given a TEXT value, return its byte length. Given NULL, return NULL. Any other input is an error.","title":"Two-bit encoding for nucleotide sequences"},{"location":"guide_helpers/#json1-and-uint-extensions","text":"The Genomics Extension bundles the SQLite developers' JSON1 extension and enables it automatically. By convention, JSON object columns should be named *_json and JSON array columns should be named *_jsarray. The JSON1 functions can be used with generated columns to effectively allow indexing of JSON-embedded fields. The UINT collating sequence is also bundled. This can be useful to make e.g. ORDER BY chromosome COLLATE UINT put 'chr2' before 'chr10'.","title":"JSON1 and UINT extensions"},{"location":"guide_helpers/#attach-genomicsqlite-database","text":"\u21aa GenomicSQLite Attach: Generate a string containing a series of SQL statements to execute on an existing database connection in order to ATTACH a GenomicSQLite database under a given schema name. The main connection may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'any.db' , uri = True ) attach_sql = genomicsqlite . attach_sql ( dbconn , 'compressed.db' , 'db2' ) # attach_sql() also takes configuration keyword arguments like # genomicsqlite.connect() dbconn . executescript ( attach_sql ) # compressed.db now attached as db2 Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:any.db\" ); String attachSql = GenomicSQLite . attachSQL ( dbconn , \"compressed.db\" , \"db2\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( attachSql ); // compressed.db now attached as db2 Rust let genomicsqlite_options = json :: object :: Object :: new (); // If needed, trigger initial load of Genomics Extension prior to // opening other connections let nop = genomicsqlite :: open ( \":memory:\" , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; // dbconn: rusqlite::Connection with SQLITE_OPEN_URI let attach_sql = dbconn . genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , & genomicsqlite_options ) ? ; dbconn . execute_batch ( & attach_sql ) ? ; // compressed.db is now attached as db2 SQLiteCpp std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); SQLite :: Database dbconn ( \"any.db\" , SQLITE_OPEN_URI ); dbconn . exec ( attach_sql ); // compressed.db now attached as db2 C++ std :: string GenomicSQLiteAttachSQL ( const std :: string & dbfile , const std :: string & schema_name , const std :: string & config_json = \"{}\" ); std :: string attach_sql = GenomicSQLiteAttachSQL ( \"compressed.db\" , \"db2\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , attach_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // compressed.db now attached as db2 C char * genomicsqlite_attach_sql ( const char * dbfile , const char * schema_name , const char * config_json ); char * attach_sql = genomicsqlite_attach_sql ( \"compressed.db\" , \"db2\" , \"{}\" ); if ( * attach_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , attach_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( attach_sql ); /* compressed.db now attached as db2 */ \u2757 The file and schema names are textually pasted into a template SQL script. Take care to prevent SQL injection, if they're in any way determined by external input.","title":"Attach GenomicSQLite database"},{"location":"guide_helpers/#compress-existing-sqlite3-database","text":"\u21aa GenomicSQLite Vacuum Into: Generate a string containing a series of SQL statements to execute on an existing database in order to copy it into a new compressed & defragmented file. The source database may be a plain, uncompressed SQLite3 database, as long as (i) it was opened with the SQLITE_OPEN_URI flag or language equivalent and (ii) the Genomics Extension is loaded in the executing program. Python dbconn = sqlite3 . connect ( 'existing.db' , uri = True ) vacuum_sql = genomicsqlite . vacuum_into_sql ( dbconn , 'compressed.db' ) # vacuum_into_sql() also takes configuration keyword arguments like # genomicsqlite.connect() to control compression level & page sizes dbconn . executescript ( vacuum_sql ) dbconn2 = genomicsqlite . connect ( 'compressed.db' ) Java // If needed, no-op to trigger initial load of Genomics Extension: DriverManager . getConnection ( \"jdbc:genomicsqlite::memory:\" ); Connection dbconn = DriverManager . getConnection ( \"jdbc:sqlite:existing.db\" ); String vacuumSql = GenomicSQLite . vacuumIntoSQL ( dbconn , \"compressed.db\" , \"{}\" ); // config_json ^^^^ dbconn . createStatement (). executeUpdate ( vaccumSql ); Connection dbconn2 = DriverManager . getConnection ( \"jdbc:genomicsqlite:compressed.db\" ); Rust let genomicsqlite_options = json :: object :: Object :: new (); // If needed, trigger initial load of Genomics Extension prior to // opening other connections let nop = genomicsqlite :: open ( \":memory:\" , OpenFlags :: SQLITE_OPEN_CREATE | OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; // dbconn: rusqlite::Connection with SQLITE_OPEN_URI let vacuum_sql = dbconn . genomicsqlite_vacuum_into_sql ( \"compressed.db\" , & genomicsqlite_options ) ? ; dbconn . execute_batch ( & vacuum_sql ) ? ; let dbconn2 = genomicsqlite :: open ( \"compressed.db\" , OpenFlags :: SQLITE_OPEN_READ_WRITE , & genomicsqlite_options ) ? ; SQLiteCpp std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); SQLite :: Database dbconn ( \"existing.db\" , SQLITE_OPEN_READONLY | SQLITE_OPEN_URI ); dbconn . exec ( vacuum_sql ); auto dbconn2 = GenomicSQLiteOpen ( \"compressed.db\" ); C++ std :: string GenomicSQLiteVacuumIntoSQL ( const std :: string & dest_filename , const std :: string & config_json = \"{}\" ); std :: string vacuum_sql = GenomicSQLiteVacuumIntoSQL ( \"compressed.db\" ); // sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db // with SQLITE_OPEN_URI char * errmsg = nullptr ; int rc = sqlite3_exec ( dbconn , vacuum_sql . c_str (), nullptr , nullptr , & errmsg ); // check rc, free errmsg // rc = GenomicSQLiteOpen(\"compressed.db\", ...); C char * genomicsqlite_vacuum_into_sql ( const char * dest_filename , const char * config_json ); char * vacuum_sql = genomicsqlite_vacuum_into_sql ( \"compressed.db\" , \"{}\" ); if ( * vacuum_sql ) { char * errmsg = 0 ; /* sqlite3* dbconn opened using sqlite3_open_v2() on some existing.db * with SQLITE_OPEN_URI */ int rc = sqlite3_exec ( dbconn , vacuum_sql , 0 , 0 , & errmsg ); /* check rc, free errmsg */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( vacuum_sql ); /* genomicsqlite_open(\"compressed.db\", ...); */","title":"Compress existing SQLite3 database"},{"location":"guide_helpers/#genomics-extension-version","text":"SQL SELECT genomicsqlite_version () Python genomicsqlite . __version__ Java String genomicsqliteVersion = GenomicSQLite . version ( dbconn ); Rust let v : String = dbconn . genomicsqlite_version (); C++ std :: string GenomicSQLiteVersion (); C char * genomicsqlite_version (); /* result to be sqlite3_free() */","title":"Genomics Extension version"},{"location":"internals/","text":"Genomics Extension Internals Compression layer SQLite's file format divides the database file into a number of fixed-size pages, and it makes I/O requests one page at a time. The compression layer is a VFS extension intermediating these requests, using Zstandard to compress pages as they're written out and decompress as they're read back in. It uses background thread pools both to parallelize page compression and to \"prefetch\" during sequential scans. The compressed pages are now variable length, yet still must be stored in a disk file that can be randomly accessed and updated. To solve this, the compression layer uses an \"outer\" SQLite3 database, effectively nesting one database inside another. Where SQLite would write database page #P at offset P \u00d7 page_size in the disk file, instead INSERT INTO outer_page_table(rowid,data) VALUES(P,compressed_inner_page) , and later SELECT data FROM outer_page_table WHERE rowid=P . Outer database transactions maintain ACID reliability (use at your own risk). Genomic Range Index The GenomicSQLite GRI is a conventional multi-column B-tree index, organized so that feature overlap with any length distribution is detectable using a series of SQL \"between tuples\" queries. Partition the features into \"levels\" according to their length. Each level L for 0 \u2264 L < 16 consists of all features with 16 L-1 < length \u2264 16 L . (Level 0 has features of length 0 or 1.) The GRI is a multi-column SQL index on each feature's: (chrom, level, position, length) The features on level L overlapping a query range (qchrom, qpos, qend) are: ((chrom, level, position) BETWEEN (qchrom, L , qbeg - 16 L ) AND (qchrom, L , qend)) AND position+length \u2265 qbeg. SQLite understands this query more-or-less as shown, and plans efficiently to (i) search the index B-tree for the first entry whose tuple (chrom, level, position) \u2265 (qchrom, L , qbeg - 16 L ), (ii) scan while it's \u2264 (qchrom, L , qend), and (iii) apply the last filter. We search higher levels over exponentially wider position ranges, but they only harbor features of comparable length. Union the disjoint results for L between 0 and 15. We can optimize queries to \"fan out\" to fewer than 16 levels (which gratuitously admit lengths up to 2 60 nt) by first figuring out the lowest and highest level actually occupied by the indexed features (floor and ceiling). This can be learned during amortized query planning, using a few quick B-tree searches per chromosome in the existing index ( genomic_range_index_levels() ). Or, the caller can supply looser bounds based on lengthiest chromosome or other prior knowledge. The fan-out is only 3 or 4 in many practical datasets. Sometimes it's inflated by a few outlier short features, which e.g. make the bottommost occupied level 0 or 1 instead of 2 or 3. In that case, we can simply push the short outliers up to a higher level \u2014 that's the floor argument to Create Genomic Range Index SQL . One last detail: we negate the level number L in the B-tree index, so that when it's built up in genomic range order, smaller (typically more-numerous) features tend to insert into the rightmost leaf \u2014 that's usually faster than interior insertions. This scheme combines the main ideas from Ensembl's query based on maximum feature length with the multi-level binning used in UCSC Genome Browser , BAI , tabix , and bedtools . It addresses the former's sensitivity to outlier lengthy features. And it's simpler than the latter, without effective constraints on feature/chromosome length. A downside is that the index takes more space, storing four values per feature. Feature length could be omitted from the B-tree index to save space at the cost of query speed. Including it lets our index query determine the exact result set without accessing the main table at all. chrom and level could be consolidated into one integer with some loss of flexibility. SQLite storage uses a variable-length integer encoding anyway. Example query Here's a GRI query on levels 0-2 using statement parameters (?1, ?2, ?3) = (queryChrom, queryBeg, queryEnd). ( SELECT _rowid_ FROM ( SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 2 ,( ? 2 ) - 0 x100 ) AND (( ? 1 ), - 2 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 1 ,( ? 2 ) - 0 x10 ) AND (( ? 1 ), - 1 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 0 ,( ? 2 ) - 0 x1 ) AND (( ? 1 ), - 0 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 )) ORDER BY _rowid_ ) The _gri_* columns are virtual generated columns added to the table based on the feature coordinate expressions specified during GRI creation. The no-op subtraction of 0 from qend was found to be needed for SQLite to use the intended query plan in all cases. So too with the repetition of the length filter constraint, which looks like it should be factored out to the outer SELECT. \ud83e\udd37 Generating subquery SQL The genomic_range_rowids() SQL function implementation just executes a query like the above and streams results back to the caller, taking care to minimize overhead by reusing prepared SQLite statements. The extension also exposes the routine to generate the SQL subquery, which you can textually paste into your SQL in place of the genomic_range_rowids() invocation. This direct method is more efficient, given proper use of prepared statements, because everything compiles into one pure SQLite bytecode program , instead of context-switching into the extension for each result. \u21aa Genomic Range Rowids SQL : Generate a string containing a parenthesized SELECT query on a GRI-indexed table, which when executed yields a rowid result set identifying the overlapping features. This is typically pasted as a subquery within a larger query that retrieves the result rows for further filtering/analysis. Python query = ( 'SELECT * FROM tableName WHERE tableName._rowid_ IN ' + genomicsqlite . genomic_range_rowids_sql ( dbconn , 'tableName' , # defaults: qrid = '?1' , qbeg = '?2' , qend = '?3' , ceiling =- 1 , floor =- 1 ) ) cursor = dbconn . execute ( query , ( 'chr12' , 111803912 , 111804012 )) SQLiteCpp std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // SQLite::Database* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn -> getHandle ()); SQLite :: Statement stmt ( * dbconn , query ); stmt . bindNoCopy ( 1 , \"chr12\" ); stmt . bind ( 2 , ( sqlite3_int64 ) 111803912 ); stmt . bind ( 3 , ( sqlite3_int64 ) 111804012 ); while ( stmt . executeStep ()) { // process row } C++ std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // sqlite3* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn ); // Omitted for brevity: // Compile query using sqlite3_prepare_v3() // Bind query range parameters using sqlite3_bind_{text,int64}() // Step through results as usual with sqlite3_step() C char * genomic_range_rowids_sql ( const char * indexed_table , sqlite3 * dbconn , const char * qrid , /* null defaults to \"?1\" */ const char * qbeg , /* null defaults to \"?2\" */ const char * qend , /* null defaults to \"?3\" */ int ceiling , int floor /* set these to -1, not 0! */ ); /* sqlite3* dbconn */ char * subquery = genomic_range_rowids_sql ( \"tableName\" , dbconn , 0 , 0 , 0 , - 1 , - 1 ); if ( * subquery ) { /* Omitted for brevity: * Append subquery to \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" * Compile query using sqlite3_prepare_v3() * Bind query range parameters using sqlite3_bind_{text,int64}() * Step through results as usual with sqlite3_step() */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( subquery ); Following the name of the indexed table to be queried , the routine takes three arguments supplying the desired range to query it for (queryChrom, queryBegin, queryEnd). These arguments default to ?1 , ?2 , and ?3 , sourcing the first three bound parameters of the top-level SQL query. They can be overridden to: other numbered or named parameter placeholders literal SQL values names of columns in other tables being joined simple expressions involving any of the above \u2757 The table name and expressions are textually pasted into a SQL template. Take care to prevent SQL injection, if they're in any way determined by external input. Unlike genomic_range_rowids() , the genomic_range_rowids_sql subquery generator defaults to level bounds auto-detected from the table at the time of subquery generation. \u2757 If the generated SQL query will be reused on a table that may have changed in the meantime, then ceiling & floor should be overidden based on the maximum and minimum possible feature length (ceiling=15 and floor=0 for full generality). If your language bindings don't include a wrapper for this routine, you can use the database connection to get the text result of SELECT genomic_range_rowids_sql(tableName[, qrid, qbeg, qend[, ceiling, floor]]) .","title":"Internals"},{"location":"internals/#genomics-extension-internals","text":"","title":"Genomics Extension Internals"},{"location":"internals/#compression-layer","text":"SQLite's file format divides the database file into a number of fixed-size pages, and it makes I/O requests one page at a time. The compression layer is a VFS extension intermediating these requests, using Zstandard to compress pages as they're written out and decompress as they're read back in. It uses background thread pools both to parallelize page compression and to \"prefetch\" during sequential scans. The compressed pages are now variable length, yet still must be stored in a disk file that can be randomly accessed and updated. To solve this, the compression layer uses an \"outer\" SQLite3 database, effectively nesting one database inside another. Where SQLite would write database page #P at offset P \u00d7 page_size in the disk file, instead INSERT INTO outer_page_table(rowid,data) VALUES(P,compressed_inner_page) , and later SELECT data FROM outer_page_table WHERE rowid=P . Outer database transactions maintain ACID reliability (use at your own risk).","title":"Compression layer"},{"location":"internals/#genomic-range-index","text":"The GenomicSQLite GRI is a conventional multi-column B-tree index, organized so that feature overlap with any length distribution is detectable using a series of SQL \"between tuples\" queries. Partition the features into \"levels\" according to their length. Each level L for 0 \u2264 L < 16 consists of all features with 16 L-1 < length \u2264 16 L . (Level 0 has features of length 0 or 1.) The GRI is a multi-column SQL index on each feature's: (chrom, level, position, length) The features on level L overlapping a query range (qchrom, qpos, qend) are: ((chrom, level, position) BETWEEN (qchrom, L , qbeg - 16 L ) AND (qchrom, L , qend)) AND position+length \u2265 qbeg. SQLite understands this query more-or-less as shown, and plans efficiently to (i) search the index B-tree for the first entry whose tuple (chrom, level, position) \u2265 (qchrom, L , qbeg - 16 L ), (ii) scan while it's \u2264 (qchrom, L , qend), and (iii) apply the last filter. We search higher levels over exponentially wider position ranges, but they only harbor features of comparable length. Union the disjoint results for L between 0 and 15. We can optimize queries to \"fan out\" to fewer than 16 levels (which gratuitously admit lengths up to 2 60 nt) by first figuring out the lowest and highest level actually occupied by the indexed features (floor and ceiling). This can be learned during amortized query planning, using a few quick B-tree searches per chromosome in the existing index ( genomic_range_index_levels() ). Or, the caller can supply looser bounds based on lengthiest chromosome or other prior knowledge. The fan-out is only 3 or 4 in many practical datasets. Sometimes it's inflated by a few outlier short features, which e.g. make the bottommost occupied level 0 or 1 instead of 2 or 3. In that case, we can simply push the short outliers up to a higher level \u2014 that's the floor argument to Create Genomic Range Index SQL . One last detail: we negate the level number L in the B-tree index, so that when it's built up in genomic range order, smaller (typically more-numerous) features tend to insert into the rightmost leaf \u2014 that's usually faster than interior insertions. This scheme combines the main ideas from Ensembl's query based on maximum feature length with the multi-level binning used in UCSC Genome Browser , BAI , tabix , and bedtools . It addresses the former's sensitivity to outlier lengthy features. And it's simpler than the latter, without effective constraints on feature/chromosome length. A downside is that the index takes more space, storing four values per feature. Feature length could be omitted from the B-tree index to save space at the cost of query speed. Including it lets our index query determine the exact result set without accessing the main table at all. chrom and level could be consolidated into one integer with some loss of flexibility. SQLite storage uses a variable-length integer encoding anyway.","title":"Genomic Range Index"},{"location":"internals/#example-query","text":"Here's a GRI query on levels 0-2 using statement parameters (?1, ?2, ?3) = (queryChrom, queryBeg, queryEnd). ( SELECT _rowid_ FROM ( SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 2 ,( ? 2 ) - 0 x100 ) AND (( ? 1 ), - 2 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 1 ,( ? 2 ) - 0 x10 ) AND (( ? 1 ), - 1 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 ) UNION ALL SELECT _rowid_ FROM reads INDEXED BY reads__gri WHERE ( reads . _gri_rid , reads . _gri_lvl , reads . _gri_beg ) BETWEEN (( ? 1 ), - 0 ,( ? 2 ) - 0 x1 ) AND (( ? 1 ), - 0 ,( ? 3 ) - 0 ) AND ( reads . _gri_beg + reads . _gri_len ) >= ( ? 2 )) ORDER BY _rowid_ ) The _gri_* columns are virtual generated columns added to the table based on the feature coordinate expressions specified during GRI creation. The no-op subtraction of 0 from qend was found to be needed for SQLite to use the intended query plan in all cases. So too with the repetition of the length filter constraint, which looks like it should be factored out to the outer SELECT. \ud83e\udd37","title":"Example query"},{"location":"internals/#generating-subquery-sql","text":"The genomic_range_rowids() SQL function implementation just executes a query like the above and streams results back to the caller, taking care to minimize overhead by reusing prepared SQLite statements. The extension also exposes the routine to generate the SQL subquery, which you can textually paste into your SQL in place of the genomic_range_rowids() invocation. This direct method is more efficient, given proper use of prepared statements, because everything compiles into one pure SQLite bytecode program , instead of context-switching into the extension for each result. \u21aa Genomic Range Rowids SQL : Generate a string containing a parenthesized SELECT query on a GRI-indexed table, which when executed yields a rowid result set identifying the overlapping features. This is typically pasted as a subquery within a larger query that retrieves the result rows for further filtering/analysis. Python query = ( 'SELECT * FROM tableName WHERE tableName._rowid_ IN ' + genomicsqlite . genomic_range_rowids_sql ( dbconn , 'tableName' , # defaults: qrid = '?1' , qbeg = '?2' , qend = '?3' , ceiling =- 1 , floor =- 1 ) ) cursor = dbconn . execute ( query , ( 'chr12' , 111803912 , 111804012 )) SQLiteCpp std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // SQLite::Database* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn -> getHandle ()); SQLite :: Statement stmt ( * dbconn , query ); stmt . bindNoCopy ( 1 , \"chr12\" ); stmt . bind ( 2 , ( sqlite3_int64 ) 111803912 ); stmt . bind ( 3 , ( sqlite3_int64 ) 111804012 ); while ( stmt . executeStep ()) { // process row } C++ std :: string GenomicRangeRowidsSQL ( const std :: string & indexed_table , sqlite3 * dbconn , const std :: string & qrid = \"?1\" , const std :: string & qbeg = \"?2\" , const std :: string & qend = \"?3\" , int ceiling = - 1 , int floor = - 1 ); // sqlite3* dbconn std :: string query = \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" + GenomicRangeRowidsSQL ( \"tableName\" , dbconn ); // Omitted for brevity: // Compile query using sqlite3_prepare_v3() // Bind query range parameters using sqlite3_bind_{text,int64}() // Step through results as usual with sqlite3_step() C char * genomic_range_rowids_sql ( const char * indexed_table , sqlite3 * dbconn , const char * qrid , /* null defaults to \"?1\" */ const char * qbeg , /* null defaults to \"?2\" */ const char * qend , /* null defaults to \"?3\" */ int ceiling , int floor /* set these to -1, not 0! */ ); /* sqlite3* dbconn */ char * subquery = genomic_range_rowids_sql ( \"tableName\" , dbconn , 0 , 0 , 0 , - 1 , - 1 ); if ( * subquery ) { /* Omitted for brevity: * Append subquery to \"SELECT * FROM tableName WHERE tableName._rowid_ IN \" * Compile query using sqlite3_prepare_v3() * Bind query range parameters using sqlite3_bind_{text,int64}() * Step through results as usual with sqlite3_step() */ } else { /* see calling convention discussed in previous examples */ } sqlite3_free ( subquery ); Following the name of the indexed table to be queried , the routine takes three arguments supplying the desired range to query it for (queryChrom, queryBegin, queryEnd). These arguments default to ?1 , ?2 , and ?3 , sourcing the first three bound parameters of the top-level SQL query. They can be overridden to: other numbered or named parameter placeholders literal SQL values names of columns in other tables being joined simple expressions involving any of the above \u2757 The table name and expressions are textually pasted into a SQL template. Take care to prevent SQL injection, if they're in any way determined by external input. Unlike genomic_range_rowids() , the genomic_range_rowids_sql subquery generator defaults to level bounds auto-detected from the table at the time of subquery generation. \u2757 If the generated SQL query will be reused on a table that may have changed in the meantime, then ceiling & floor should be overidden based on the maximum and minimum possible feature length (ceiling=15 and floor=0 for full generality). If your language bindings don't include a wrapper for this routine, you can use the database connection to get the text result of SELECT genomic_range_rowids_sql(tableName[, qrid, qbeg, qend[, ceiling, floor]]) .","title":"Generating subquery SQL"}]}